{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UdVo-R_cLISp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIX_lLO_LISq"
      },
      "source": [
        "##  Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLrYwK1CLW0E",
        "outputId": "e40de000-b54a-486c-ead0-b965587d865b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi3mkph2LISr",
        "outputId": "9844bdcb-2aa3-4918-86d9-1675510ace3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(512,) (512,)\n",
            "(1, 512) (1, 512)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.99951637]], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word1 = 'black'\n",
        "word2 = 'brown'\n",
        "pos1 = 2\n",
        "pos2 = 10\n",
        "\n",
        "#‘text.txt’ file\n",
        "sample = open(\"text.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "# processing escape characters\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "\n",
        "# dimension of the vector for the model\n",
        "d_model = 512\n",
        "\n",
        "# sentence parsing\n",
        "for i in nltk.tokenize.sent_tokenize(f):\n",
        "\ttemp = []\n",
        "\t# tokenize the sentence into words\n",
        "\tfor j in nltk.tokenize.word_tokenize(i):\n",
        "\t\ttemp.append(j.lower())\n",
        "\tdata.append(temp)\n",
        "\n",
        "model = Word2Vec(data, min_count=1, vector_size=d_model, window=5, sg=1)\n",
        "\n",
        "# getting word embeddings with trained model of Word2Vec\n",
        "a = model.wv[word1]\n",
        "b = model.wv[word2]\n",
        "print(a.shape, b.shape)\n",
        "\n",
        "aa = a.reshape(1,512)\n",
        "ba = b.reshape(1, 512)\n",
        "print(aa.shape, ba.shape)\n",
        "\n",
        "# cosine similarity\n",
        "cosine_similarity(aa, ba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCbIwyPnMDmb"
      },
      "source": [
        "0.9995 similarity between word embeddings of word 'black' and 'brown'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rC7OB3NeMX2K"
      },
      "outputs": [],
      "source": [
        "def positional_vector(pos, d_model):\n",
        "    \"\"\"Function to return positional information added to the word embeddings\n",
        "\n",
        "    Arguments:\n",
        "        pos -- The position of the word in the sentence/ corpus\n",
        "    \"\"\"\n",
        "    pv = np.random.randn(1,512)\n",
        "    d_model = 512\n",
        "    # even for sine function, odd for cosine\n",
        "    for i in range(0,512, 2):\n",
        "        pv[0][i] = np.sin(pos/10000**((2*i)/d_model))\n",
        "        pv[0][i+1] = np.cos(pos/10000**((2*i)/d_model))\n",
        "\n",
        "    return pv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0hvpL9_hpxb",
        "outputId": "0c76b125-3507-46ae-84d2-546e9d7e6311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 512)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.randn(1,512).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV0mygGjLISs",
        "outputId": "202c7059-fa21-4815-b51d-e9d07cf12fb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.86000133]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting positional vector\n",
        "# for the information of the position of the word\n",
        "pe1 = aa.copy() # with the same shape as the word embeddings\n",
        "pe2 = ba.copy()\n",
        "cosine_similarity(positional_vector(2, d_model), positional_vector(10, d_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLkOepYCOb-y"
      },
      "source": [
        "## Hence, 86.00 % similarity between positions of the words!\n",
        "\n",
        "\n",
        "Add 'small values representing positional vector' to the word embeddings so that positions are taken into account!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_oajgeGzPXah"
      },
      "outputs": [],
      "source": [
        "# for final positional encoding\n",
        "paa1 = aa.copy()\n",
        "pba2 = ba.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dC52jo4qLuws"
      },
      "outputs": [],
      "source": [
        "# positional encoding combining both positional vector and word embeddings\n",
        "def positional_encoding(pos, d_model, paa):\n",
        "    \"\"\"\n",
        "        Function to return positional encoding with combination of word embeddings\n",
        "        and positional vector\n",
        "    Arguments:\n",
        "        pos: Position of the word\n",
        "        pe: Positional vector variable of that shape\n",
        "        d_model: Dimension of the vector required for the model\n",
        "        paa: Variable to hold final positional Encoding of the word\n",
        "    \"\"\"\n",
        "    pv = np.random.randn(1,d_model)\n",
        "    for i in range(0,d_model, 2):\n",
        "        pv[0][i] = np.sin(pos/10000**((2*i)/d_model))\n",
        "\n",
        "        # Value of the word embedding is increased by the product with sqrt of 512\n",
        "        # Simple addition of the positional information to the word embedding\n",
        "        paa[0][i] = (paa[0][i] * np.sqrt(d_model)) + pv[0][i]\n",
        "\n",
        "        pv[0][i+1] = np.cos(pos/10000**((2*i)/d_model))\n",
        "        paa[0][i+1] = (paa[0][i+1] * np.sqrt(d_model)) + pv[0][i+1]\n",
        "\n",
        "    return paa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSzQcJIZNyr6",
        "outputId": "daddc334-ce1b-416e-de70-a46f1b634894"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.9622495]], dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(positional_encoding(2, d_model, paa1), positional_encoding(10, d_model, pba2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQhfF18EQZrL"
      },
      "source": [
        "Hence, the similarity between **Positional Encoding** between the words 'black' and 'brown' is 95.97%.\n",
        "\n",
        "\n",
        "*Inclusion of word embeddings with positional vector*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
