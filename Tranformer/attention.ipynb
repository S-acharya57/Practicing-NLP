{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Represent the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]] (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# d_model = 4\n",
    "# 3 inputs\n",
    "d_k = 3\n",
    "x = np.array([[1.0, 0.0, 1.0, 0.0],\n",
    "             [0.0, 2.0, 0.0, 2.0], \n",
    "             [1.0, 1.0, 1.0, 1.0]])\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (Q) 3 x d_model=4\n",
      " [[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]] (4, 3)\n",
      "Weights (K) 3 x d_model=4\n",
      " [[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]] (4, 3)\n",
      "Weights (V) 3 x d_model=4\n",
      " [[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]] (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# even though d_k = 64, lets scale it down to 3 for better visualization\n",
    "# weights for query(Q)\n",
    "w_query = np.array([[1,0,1],\n",
    "                    [1,0,0],\n",
    "                    [0,0,1],\n",
    "                    [0,1,1]])\n",
    "print(f'Weights (Q) 3 x d_model=4\\n', w_query, w_query.shape)\n",
    "\n",
    "# weights for key(K)\n",
    "w_key = np.array([[0,0,1],\n",
    "                  [1,1,0],\n",
    "                  [0,1,0],\n",
    "                  [1,1,0]])\n",
    "print(f'Weights (K) 3 x d_model=4\\n', w_key, w_key.shape)\n",
    "\n",
    "# weights for value(V)\n",
    "w_value = np.array([[0,2,0],\n",
    "                    [0,3,0],\n",
    "                    [1,0,3],\n",
    "                    [1,1,0]])\n",
    "print(f'Weights (V) 3 x d_model=4\\n', w_value, w_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Matrix Multiplication to get Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query(Q):\n",
      " [[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n",
      "Key(K):\n",
      " [[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n",
      "Value(V):\n",
      " [[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.matmul(x, w_query)\n",
    "K = np.matmul(x, w_key)\n",
    "V = np.matmul(x, w_value)\n",
    "print(f'Query(Q):\\n {Q}')\n",
    "print(f'Key(K):\\n {K}')\n",
    "print(f'Value(V):\\n {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Equation for Attention Scores\n",
    "\n",
    "$$ \\large\n",
    "Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaled Attention Scores (Q, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores:\n",
      " [[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "#d_k = sqrt(3) but assuming it to be 1\n",
    "root_dk = 1\n",
    "attention_scores = (Q@ K.T) / root_dk\n",
    "print(f'attention_scores:\\n {attention_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaled Softmax Attention Scores for Each Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores:\n",
      " [[6.33789383e-02 4.68310531e-01 4.68310531e-01]\n",
      " [6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      " [2.95387223e-04 8.80536902e-01 1.19167711e-01]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(attention_scores.shape[1]):\n",
    "    attention_scores[i] = softmax(attention_scores[i])\n",
    "print(f'attention_scores:\\n {attention_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is:\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "attentions = []\n",
    "print(f'Attention is:')\n",
    "for i in range(d_k):\n",
    "    # print(f\"Attention Scores shape: {attention_scores[i].shape}\")\n",
    "    attention_scores[i] = attention_scores[i].reshape(1,-1)\n",
    "    # print(f\"Attention Scores shape After Reshape: {attention_scores[i].shape}\")\n",
    "    attentions.append(attention_scores[0][i] * V[i])\n",
    "    print(attentions[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of attention is calculated attention of Q, K and V for each input. \\\n",
    "i.e 1st row -> For x1 input and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sum up the result attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Attentions is: \n",
      "[0.3802736299982257, 4.683105308334811, 5.151415839168293]\n"
     ]
    }
   ],
   "source": [
    "attention_input1 = [a+b+c for a,b,c in attentions]\n",
    "print(f'Sum of Attentions is: \\n{attention_input1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. For dimension of the model as d_k = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.314707299976895"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(attention_head1[i][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64)\n",
      "[29.797725576291956, 28.84816890358251, 29.314707299976895]\n"
     ]
    }
   ],
   "source": [
    "attention_head1 = np.random.random((3,64))\n",
    "print(attention_head1.shape)\n",
    "attention_inputs = []\n",
    "for i in range(attention_head1.shape[0]):\n",
    "    attention_inputs.append(np.sum(attention_head1[i][:]))\n",
    "print(attention_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
