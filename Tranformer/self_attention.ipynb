{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "Self attention is a mechanism that enhances the information content of an input embedding by including information about the input's context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 1,\n",
       " 'Lately': 2,\n",
       " 'cake.': 3,\n",
       " 'cakes': 4,\n",
       " 'cakes.': 5,\n",
       " 'chocolate': 6,\n",
       " 'love': 7,\n",
       " 'specially': 8,\n",
       " 'to': 9,\n",
       " 'try': 10,\n",
       " 'vanilla': 11,\n",
       " 'want': 12}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love cakes, specially chocolate cakes. Lately, I want to try vanilla cake.\"\n",
    "\n",
    "dict_words = {s:i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "dict_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  7,  4,  8,  6,  5,  2,  1, 12,  9, 10, 11,  3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign integer index to each word in the sentence\n",
    "sentence_indices = torch.tensor([dict_words[word] for word in sentence.replace(',', '').split()])\n",
    "sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(13,16) \n",
    "embedded_sentence = embed(sentence_indices)\n",
    "embedded_sentence.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention (Scaled Dot Product Attention)\n",
    "\n",
    "$W_q, W_k, W_v$ are weight matrices that are adjusted during model training.\n",
    "Key, Query, Value sequences are obtained by matrix multiplication between the weight matrices **W** and input embeddings **x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 16]), torch.Size([16, 16]), torch.Size([20, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension of word, and the vectors\n",
    "d_word = embedded_sentence.shape[1]\n",
    "d_q = 16\n",
    "d_k = 16\n",
    "d_v = 20 \n",
    "\n",
    "# initialize the weight matrices\n",
    "W_q = torch.nn.Parameter(torch.randn(d_q, d_word))\n",
    "W_k = torch.nn.Parameter(torch.randn(d_k, d_word))\n",
    "W_v = torch.nn.Parameter(torch.randn(d_v, d_word))\n",
    "\n",
    "W_q.shape, W_k.shape, W_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting attention scores and final weighted values for second element of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([16]), torch.Size([20]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting attention-vector for second input element\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = torch.matmul(W_q, x_2)\n",
    "key_2 = torch.matmul(W_k, x_2)\n",
    "value_2 = torch.matmul(W_v, x_2)\n",
    "\n",
    "query_2.shape, key_2.shape, value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 16]), torch.Size([16, 13]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_k.shape, embedded_sentence.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 16]), torch.Size([13, 20]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting keys and values for the remaining sequence too\n",
    "keys = W_k.matmul(embedded_sentence.T).T \n",
    "values = W_v.matmul(embedded_sentence.T).T\n",
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(27.5843, grad_fn=<DotBackward0>),\n",
       " tensor(27.5843, grad_fn=<DotBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting attention scores for the second element\n",
    "attention_score_2 = query_2.matmul(key_2.T)\n",
    "\n",
    "# the second element in keys refer to the keys corresponding to the second element \n",
    "attention_score_2, query_2.matmul(keys[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 60.7147,  27.5843, -27.5929, -17.2011,  -9.4265, -39.2114,  34.1458,\n",
       "         60.7147, -38.7424,  22.7840, -62.3283, -22.1738, -63.6789],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention score for second element with respect to all words \n",
    "attention_scores_2 = query_2.matmul(keys.T) \n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_13236\\3491346819.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weights_2 = torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.9959e-01, 1.2634e-04, 1.2904e-10, 1.7338e-09, 1.2110e-08, 7.0677e-12,\n",
       "        6.5152e-04, 4.9959e-01, 7.9469e-12, 3.8048e-05, 2.1847e-14, 5.0016e-10,\n",
       "        1.5587e-14], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize with softmax, and scale with sqrt(d_k)\n",
    "# change attention scores to probabilities\n",
    "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k))) \n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9959e-01, 1.2634e-04, 1.2904e-10, 1.7338e-09, 1.2110e-08, 7.0677e-12,\n",
       "        6.5152e-04, 4.9959e-01, 7.9469e-12, 3.8048e-05, 2.1847e-14, 5.0016e-10,\n",
       "        1.5587e-14], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k)), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted values\n",
    "# input sequence with acquired attention\n",
    "weighted_values_2 = attention_weights_2.matmul(values)\n",
    "weighted_values_2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_word, d_q, d_k, d_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.randn(d_q, d_word))\n",
    "        self.W_k = torch.nn.Parameter(torch.randn(d_k, d_word))\n",
    "        self.W_v = torch.nn.Parameter(torch.randn(d_v, d_word))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query = torch.matmul(self.W_q, x)\n",
    "        key = torch.matmul(self.W_k, x)\n",
    "        value = torch.matmul(self.W_v, x)\n",
    "        \n",
    "        attention_scores = query.matmul(key.T)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores/torch.sqrt(torch.tensor(d_k)), dim=0)\n",
    "        weighted_values = attention_weights.matmul(value.T)\n",
    "        return weighted_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
