{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "Self attention is a mechanism that enhances the information content of an input embedding by including information about the input's context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 1,\n",
       " 'Lately': 2,\n",
       " 'cake.': 3,\n",
       " 'cakes': 4,\n",
       " 'cakes.': 5,\n",
       " 'chocolate': 6,\n",
       " 'love': 7,\n",
       " 'specially': 8,\n",
       " 'to': 9,\n",
       " 'try': 10,\n",
       " 'vanilla': 11,\n",
       " 'want': 12}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love cakes, specially chocolate cakes. Lately, I want to try vanilla cake.\"\n",
    "\n",
    "dict_words = {s:i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "dict_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  7,  4,  8,  6,  5,  2,  1, 12,  9, 10, 11,  3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign integer index to each word in the sentence\n",
    "sentence_indices = torch.tensor([dict_words[word] for word in sentence.replace(',', '').split()])\n",
    "sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(13,16) \n",
    "embedded_sentence = embed(sentence_indices)\n",
    "embedded_sentence.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention (Scaled Dot Product Attention)\n",
    "\n",
    "$W_q, W_k, W_v$ are weight matrices that are adjusted during model training.\n",
    "Key, Query, Value sequences are obtained by matrix multiplication between the weight matrices **W** and input embeddings **x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 16]), torch.Size([16, 16]), torch.Size([20, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension of word, and the vectors\n",
    "d_word = embedded_sentence.shape[1]\n",
    "d_q = 16\n",
    "d_k = 16\n",
    "d_v = 20 \n",
    "\n",
    "# initialize the weight matrices\n",
    "W_q = torch.nn.Parameter(torch.randn(d_q, d_word))\n",
    "W_k = torch.nn.Parameter(torch.randn(d_k, d_word))\n",
    "W_v = torch.nn.Parameter(torch.randn(d_v, d_word))\n",
    "\n",
    "W_q.shape, W_k.shape, W_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting attention scores and final weighted values for second element of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([16]), torch.Size([20]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting attention-vector for second input element\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = torch.matmul(W_q, x_2)\n",
    "key_2 = torch.matmul(W_k, x_2)\n",
    "value_2 = torch.matmul(W_v, x_2)\n",
    "\n",
    "query_2.shape, key_2.shape, value_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 16]), torch.Size([16, 13]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_k.shape, embedded_sentence.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 16]), torch.Size([13, 20]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting keys and values for the remaining sequence too\n",
    "keys = W_k.matmul(embedded_sentence.T).T \n",
    "values = W_v.matmul(embedded_sentence.T).T\n",
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_17256\\1193251138.py:2: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3641.)\n",
      "  attention_score_2 = query_2.matmul(key_2.T)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-35.4803, grad_fn=<DotBackward0>),\n",
       " tensor(-35.4803, grad_fn=<DotBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting attention scores for the second element\n",
    "attention_score_2 = query_2.matmul(key_2.T)\n",
    "\n",
    "# the second element in keys refer to the keys corresponding to the second element \n",
    "attention_score_2, query_2.matmul(keys[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 29.3837, -35.4803,  10.7918,  78.3755,  47.9072,  49.6510, -30.6851,\n",
       "         29.3837,  21.4900,  -0.9740, -71.2418,  41.8499, -28.4696],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention score for second element with respect to all words \n",
    "attention_scores_2 = query_2.matmul(keys.T) \n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_17256\\1129350789.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weights_2 = torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.7884e-06, 4.3418e-13, 4.5877e-08, 9.9863e-01, 4.9131e-04, 7.5977e-04,\n",
       "        1.4398e-12, 4.7884e-06, 6.6548e-07, 2.4218e-09, 5.6874e-17, 1.0807e-04,\n",
       "        2.5052e-12], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize with softmax, and scale with sqrt(d_k)\n",
    "# change attention scores to probabilities\n",
    "attention_weights_2 = torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k))) \n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7884e-06, 4.3418e-13, 4.5877e-08, 9.9863e-01, 4.9131e-04, 7.5977e-04,\n",
       "        1.4398e-12, 4.7884e-06, 6.6548e-07, 2.4218e-09, 5.6874e-17, 1.0807e-04,\n",
       "        2.5052e-12], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(attention_scores_2/torch.sqrt(torch.tensor(d_k)), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted values\n",
    "# input sequence with acquired attention\n",
    "weighted_values_2 = attention_weights_2.matmul(values)\n",
    "weighted_values_2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences, embedding_shape, max_sentences):\n",
    "    dict_words = {s:i for i, s in enumerate(sorted(sentences.replace(',', '').split()))}\n",
    "    # print(f'Dict words: {dict_words}')\n",
    "    sentence_indices = [dict_words[word] for word in sentences.replace(',', '').split()]\n",
    "    sentence_indices = sentence_indices[:max_sentences] + [0] * (max_sentences - len(sentence_indices))  # Pad/truncate\n",
    "    sentence_indices = torch.tensor(sentence_indices, dtype=torch.long)\n",
    "\n",
    "    embed = torch.nn.Embedding(len(dict_words)+500,embedding_shape)\n",
    "    embedded_sentence = embed(sentence_indices)\n",
    "    return embedded_sentence, dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_word, d_q, d_k, d_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.randn(d_q, d_word))\n",
    "        self.W_k = torch.nn.Parameter(torch.randn(d_k, d_word))\n",
    "        self.W_v = torch.nn.Parameter(torch.randn(d_v, d_word))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query = torch.matmul(self.W_q, x.T)\n",
    "        key = torch.matmul(self.W_k, x.T)\n",
    "        value = torch.matmul(self.W_v, x.T)\n",
    "        \n",
    "        attention_scores = query.matmul(key.T)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores/torch.sqrt(torch.tensor(d_k)), dim=0)\n",
    "        # print(f'attention_weights: {attention_weights.shape}')\n",
    "        # print(f'value shape: {value.shape}')\n",
    "        weighted_values = attention_weights.matmul(value)\n",
    "        return weighted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, d_word, d_q, d_k, d_v, d_linear):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.attention = SelfAttention(d_word, d_q, d_k, d_v)\n",
    "        \n",
    "        # attention values will be taken mean \n",
    "        # shape will be (d_word, d_v)\n",
    "        self.fc = nn.Linear(d_linear, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        # print(f'After attention: {x.shape}')\n",
    "        # print(f'After mean, attention: {x.mean(dim=0).shape}')\n",
    "        x = self.fc(x.mean(dim=0))\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    (\"I love this product\", 1),\n",
    "    (\"This is amazing\", 1),\n",
    "    (\"I hate this\", 0),\n",
    "    (\"This is terrible\", 0),\n",
    "    (\"Absolutely fantastic\", 1),\n",
    "    (\"Horrible experience\", 0),\n",
    "    (\"Best decision ever\", 1),\n",
    "    (\"Worst thing happened\", 0),\n",
    "    (\"I really enjoyed using this\", 1),\n",
    "    (\"This was a huge disappointment\", 0),\n",
    "    (\"Highly recommend this to everyone\", 1),\n",
    "    (\"Would not buy again\", 0),\n",
    "    (\"Superb quality and performance\", 1),\n",
    "    (\"Broke after one use\", 0),\n",
    "    (\"Exceeded my expectations\", 1),\n",
    "    (\"Not worth the money\", 0),\n",
    "    (\"Great value for the price\", 1),\n",
    "    (\"Completely useless\", 0),\n",
    "    (\"Absolutely love it\", 1),\n",
    "    (\"I regret buying this\", 0),\n",
    "    (\"Best purchase ever\", 1),\n",
    "    (\"The worst experience of my life\", 0),\n",
    "    (\"Would purchase again\", 1),\n",
    "    (\"This is a waste of time\", 0),\n",
    "    (\"Fantastic product, works perfectly\", 1),\n",
    "    (\"One of the worst purchases I've made\", 0),\n",
    "    (\"So happy with this\", 1),\n",
    "    (\"Terrible service and product\", 0),\n",
    "    (\"Quality is top-notch\", 1),\n",
    "    (\"Completely broke within a week\", 0),\n",
    "    (\"Loved the fast delivery and quality\", 1),\n",
    "    (\"Would never recommend this to anyone\", 0),\n",
    "    (\"A must-have for everyone\", 1),\n",
    "    (\"Total waste of money\", 0),\n",
    "    (\"Incredible value for the price\", 1),\n",
    "    (\"Hated every part of it\", 0),\n",
    "    (\"Five stars all the way\", 1),\n",
    "    (\"A complete letdown\", 0),\n",
    "    (\"The best decision I've made\", 1),\n",
    "    (\"Worst experience I've had\", 0),\n",
    "    (\"So satisfied with my purchase\", 1),\n",
    "    (\"Not worth even a single penny\", 0),\n",
    "    (\"Fantastic build and performance\", 1),\n",
    "    (\"It failed to work as expected\", 0),\n",
    "    (\"Absolutely stunning product\", 1),\n",
    "    (\"Terrible quality, do not buy\", 0),\n",
    "    (\"Beyond my expectations\", 1),\n",
    "    (\"Disappointed with the service\", 0),\n",
    "    (\"Great overall experience\", 1),\n",
    "    (\"Product broke too soon\", 0),\n",
    "]\n",
    "\n",
    "train_sentences = [s for s, _ in train_data]\n",
    "\n",
    "test_sentences = [\n",
    "    \"This is fantastic\",\n",
    "    \"I regret buying this\",\n",
    "    \"I absolutely love it\",\n",
    "    \"Worst experience ever\",\n",
    "    \"Best purchase of my life\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this product\n",
      "1\n",
      "This is amazing\n",
      "1\n",
      "I hate this\n",
      "0\n",
      "This is terrible\n",
      "0\n",
      "Absolutely fantastic\n",
      "1\n",
      "Horrible experience\n",
      "0\n",
      "Best decision ever\n",
      "1\n",
      "Worst thing happened\n",
      "0\n",
      "I really enjoyed using this\n",
      "1\n",
      "This was a huge disappointment\n",
      "0\n",
      "Highly recommend this to everyone\n",
      "1\n",
      "Would not buy again\n",
      "0\n",
      "Superb quality and performance\n",
      "1\n",
      "Broke after one use\n",
      "0\n",
      "Exceeded my expectations\n",
      "1\n",
      "Not worth the money\n",
      "0\n",
      "Great value for the price\n",
      "1\n",
      "Completely useless\n",
      "0\n",
      "Absolutely love it\n",
      "1\n",
      "I regret buying this\n",
      "0\n",
      "Best purchase ever\n",
      "1\n",
      "The worst experience of my life\n",
      "0\n",
      "Would purchase again\n",
      "1\n",
      "This is a waste of time\n",
      "0\n",
      "Fantastic product, works perfectly\n",
      "1\n",
      "One of the worst purchases I've made\n",
      "0\n",
      "So happy with this\n",
      "1\n",
      "Terrible service and product\n",
      "0\n",
      "Quality is top-notch\n",
      "1\n",
      "Completely broke within a week\n",
      "0\n",
      "Loved the fast delivery and quality\n",
      "1\n",
      "Would never recommend this to anyone\n",
      "0\n",
      "A must-have for everyone\n",
      "1\n",
      "Total waste of money\n",
      "0\n",
      "Incredible value for the price\n",
      "1\n",
      "Hated every part of it\n",
      "0\n",
      "Five stars all the way\n",
      "1\n",
      "A complete letdown\n",
      "0\n",
      "The best decision I've made\n",
      "1\n",
      "Worst experience I've had\n",
      "0\n",
      "So satisfied with my purchase\n",
      "1\n",
      "Not worth even a single penny\n",
      "0\n",
      "Fantastic build and performance\n",
      "1\n",
      "It failed to work as expected\n",
      "0\n",
      "Absolutely stunning product\n",
      "1\n",
      "Terrible quality, do not buy\n",
      "0\n",
      "Beyond my expectations\n",
      "1\n",
      "Disappointed with the service\n",
      "0\n",
      "Great overall experience\n",
      "1\n",
      "Product broke too soon\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for sentence, label in train_data:\n",
    "    print(sentence)\n",
    "    print(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = torch.tensor([label_ for _, label_ in train_data], dtype=torch.float32)\n",
    "train_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "embedded_sentences, word_dict_ = process_sentences(' '.join(train_sentences), embedding_dim, max_len)\n",
    "embedded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (attention): SelfAttention()\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_word = embedded_sentences.shape[1]\n",
    "d_linear = max_len \n",
    "d_q = 16        # query dimension\n",
    "d_k = 16        # key dimension\n",
    "d_v = 16        # value dimension\n",
    "\n",
    "# initialize the model\n",
    "model = SentimentClassifier(d_word, d_q, d_k, d_v, d_linear)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6641170978546143\n",
      "Epoch: 0, Loss: 0.7257056832313538\n",
      "Epoch: 0, Loss: 0.6470229625701904\n",
      "Epoch: 0, Loss: 0.6427892446517944\n",
      "Epoch: 0, Loss: 0.45382237434387207\n",
      "Epoch: 1, Loss: 0.6205589175224304\n",
      "Epoch: 1, Loss: 0.5748776793479919\n",
      "Epoch: 1, Loss: 0.5451675653457642\n",
      "Epoch: 1, Loss: 0.7050160765647888\n",
      "Epoch: 1, Loss: 0.6728764772415161\n",
      "Epoch: 2, Loss: 0.7525603175163269\n",
      "Epoch: 2, Loss: 0.7572631239891052\n",
      "Epoch: 2, Loss: 0.6798258423805237\n",
      "Epoch: 2, Loss: 0.5232846140861511\n",
      "Epoch: 2, Loss: 0.6812126636505127\n",
      "Epoch: 3, Loss: 0.5486624240875244\n",
      "Epoch: 3, Loss: 0.6258867383003235\n",
      "Epoch: 3, Loss: 0.6736737489700317\n",
      "Epoch: 3, Loss: 0.713619589805603\n",
      "Epoch: 3, Loss: 0.6508697271347046\n",
      "Epoch: 4, Loss: 0.7275972366333008\n",
      "Epoch: 4, Loss: 0.5853260159492493\n",
      "Epoch: 4, Loss: 0.6316661834716797\n",
      "Epoch: 4, Loss: 0.6792041659355164\n",
      "Epoch: 4, Loss: 0.7094566226005554\n",
      "Epoch: 5, Loss: 0.6542940139770508\n",
      "Epoch: 5, Loss: 0.5303098559379578\n",
      "Epoch: 5, Loss: 0.7321962118148804\n",
      "Epoch: 5, Loss: 0.6278398036956787\n",
      "Epoch: 5, Loss: 0.6276140213012695\n",
      "Epoch: 6, Loss: 0.7225464582443237\n",
      "Epoch: 6, Loss: 0.6306439638137817\n",
      "Epoch: 6, Loss: 0.637606680393219\n",
      "Epoch: 6, Loss: 0.5875052213668823\n",
      "Epoch: 6, Loss: 0.6210556030273438\n",
      "Epoch: 7, Loss: 0.6265130639076233\n",
      "Epoch: 7, Loss: 0.629082977771759\n",
      "Epoch: 7, Loss: 0.7237281799316406\n",
      "Epoch: 7, Loss: 0.7617610692977905\n",
      "Epoch: 7, Loss: 0.6353382468223572\n",
      "Epoch: 8, Loss: 0.522940456867218\n",
      "Epoch: 8, Loss: 0.5770103931427002\n",
      "Epoch: 8, Loss: 0.40610402822494507\n",
      "Epoch: 8, Loss: 0.6824724674224854\n",
      "Epoch: 8, Loss: 0.7881709337234497\n",
      "Epoch: 9, Loss: 0.8469713926315308\n",
      "Epoch: 9, Loss: 0.6623612642288208\n",
      "Epoch: 9, Loss: 0.6055087447166443\n",
      "Epoch: 9, Loss: 0.7180569171905518\n",
      "Epoch: 9, Loss: 0.737226128578186\n",
      "Epoch: 10, Loss: 0.5548338890075684\n",
      "Epoch: 10, Loss: 0.6484953165054321\n",
      "Epoch: 10, Loss: 0.638498067855835\n",
      "Epoch: 10, Loss: 0.7265811562538147\n",
      "Epoch: 10, Loss: 0.7106284499168396\n",
      "Epoch: 11, Loss: 0.6821621656417847\n",
      "Epoch: 11, Loss: 0.6665023565292358\n",
      "Epoch: 11, Loss: 0.6779194474220276\n",
      "Epoch: 11, Loss: 0.6346654295921326\n",
      "Epoch: 11, Loss: 0.6704106330871582\n",
      "Epoch: 12, Loss: 0.7818807363510132\n",
      "Epoch: 12, Loss: 0.6865731477737427\n",
      "Epoch: 12, Loss: 0.8249815106391907\n",
      "Epoch: 12, Loss: 0.6290162801742554\n",
      "Epoch: 12, Loss: 0.6262929439544678\n",
      "Epoch: 13, Loss: 0.6455163359642029\n",
      "Epoch: 13, Loss: 0.6325510144233704\n",
      "Epoch: 13, Loss: 0.5673359632492065\n",
      "Epoch: 13, Loss: 0.5905672311782837\n",
      "Epoch: 13, Loss: 0.6803961396217346\n",
      "Epoch: 14, Loss: 0.7697226405143738\n",
      "Epoch: 14, Loss: 0.6467356085777283\n",
      "Epoch: 14, Loss: 0.6845331788063049\n",
      "Epoch: 14, Loss: 0.6456660628318787\n",
      "Epoch: 14, Loss: 0.7314997315406799\n",
      "Epoch: 15, Loss: 0.7429808378219604\n",
      "Epoch: 15, Loss: 0.7338399887084961\n",
      "Epoch: 15, Loss: 0.6084140539169312\n",
      "Epoch: 15, Loss: 0.5794994235038757\n",
      "Epoch: 15, Loss: 0.6451032161712646\n",
      "Epoch: 16, Loss: 0.6884682774543762\n",
      "Epoch: 16, Loss: 0.6378628015518188\n",
      "Epoch: 16, Loss: 0.6023942828178406\n",
      "Epoch: 16, Loss: 0.5699339509010315\n",
      "Epoch: 16, Loss: 0.6432984471321106\n",
      "Epoch: 17, Loss: 0.7231430411338806\n",
      "Epoch: 17, Loss: 0.5954669117927551\n",
      "Epoch: 17, Loss: 0.5125676393508911\n",
      "Epoch: 17, Loss: 0.5732477903366089\n",
      "Epoch: 17, Loss: 0.6619507670402527\n",
      "Epoch: 18, Loss: 0.6016156077384949\n",
      "Epoch: 18, Loss: 0.7976920008659363\n",
      "Epoch: 18, Loss: 0.7266578674316406\n",
      "Epoch: 18, Loss: 0.6397614479064941\n",
      "Epoch: 18, Loss: 0.7617052793502808\n",
      "Epoch: 19, Loss: 0.6828594207763672\n",
      "Epoch: 19, Loss: 0.585484504699707\n",
      "Epoch: 19, Loss: 0.7069588303565979\n",
      "Epoch: 19, Loss: 0.7610574960708618\n",
      "Epoch: 19, Loss: 0.4621370732784271\n",
      "Epoch: 20, Loss: 0.6526846289634705\n",
      "Epoch: 20, Loss: 0.49123281240463257\n",
      "Epoch: 20, Loss: 0.8309029936790466\n",
      "Epoch: 20, Loss: 0.5722306370735168\n",
      "Epoch: 20, Loss: 0.5524249076843262\n",
      "Epoch: 21, Loss: 0.6700277924537659\n",
      "Epoch: 21, Loss: 0.9356591105461121\n",
      "Epoch: 21, Loss: 1.004289984703064\n",
      "Epoch: 21, Loss: 1.3956971168518066\n",
      "Epoch: 21, Loss: 0.6632041931152344\n",
      "Epoch: 22, Loss: 0.7181396484375\n",
      "Epoch: 22, Loss: 0.7259282469749451\n",
      "Epoch: 22, Loss: 0.7272816896438599\n",
      "Epoch: 22, Loss: 0.5817683935165405\n",
      "Epoch: 22, Loss: 0.6543086171150208\n",
      "Epoch: 23, Loss: 0.6882727742195129\n",
      "Epoch: 23, Loss: 0.7014122605323792\n",
      "Epoch: 23, Loss: 0.7462387084960938\n",
      "Epoch: 23, Loss: 0.6325047612190247\n",
      "Epoch: 23, Loss: 0.6618306040763855\n",
      "Epoch: 24, Loss: 0.6694844365119934\n",
      "Epoch: 24, Loss: 0.6411322951316833\n",
      "Epoch: 24, Loss: 0.7215632200241089\n",
      "Epoch: 24, Loss: 0.6951215863227844\n",
      "Epoch: 24, Loss: 0.6462022662162781\n",
      "Epoch: 25, Loss: 0.6821629405021667\n",
      "Epoch: 25, Loss: 0.6096029877662659\n",
      "Epoch: 25, Loss: 0.505630373954773\n",
      "Epoch: 25, Loss: 0.5489569902420044\n",
      "Epoch: 25, Loss: 0.6539804935455322\n",
      "Epoch: 26, Loss: 0.6056163311004639\n",
      "Epoch: 26, Loss: 0.6916949152946472\n",
      "Epoch: 26, Loss: 0.6304289698600769\n",
      "Epoch: 26, Loss: 0.8869062662124634\n",
      "Epoch: 26, Loss: 0.780936598777771\n",
      "Epoch: 27, Loss: 0.662198543548584\n",
      "Epoch: 27, Loss: 0.9106060862541199\n",
      "Epoch: 27, Loss: 0.674507737159729\n",
      "Epoch: 27, Loss: 0.7173855900764465\n",
      "Epoch: 27, Loss: 0.6456207036972046\n",
      "Epoch: 28, Loss: 0.8187195062637329\n",
      "Epoch: 28, Loss: 0.7154563665390015\n",
      "Epoch: 28, Loss: 0.6771757006645203\n",
      "Epoch: 28, Loss: 0.5896280407905579\n",
      "Epoch: 28, Loss: 0.8287765383720398\n",
      "Epoch: 29, Loss: 0.7065204381942749\n",
      "Epoch: 29, Loss: 0.6754846572875977\n",
      "Epoch: 29, Loss: 0.6689525246620178\n",
      "Epoch: 29, Loss: 0.6194730401039124\n",
      "Epoch: 29, Loss: 0.8724013566970825\n",
      "Epoch: 30, Loss: 0.6839537024497986\n",
      "Epoch: 30, Loss: 0.7443742752075195\n",
      "Epoch: 30, Loss: 0.684759259223938\n",
      "Epoch: 30, Loss: 0.6814629435539246\n",
      "Epoch: 30, Loss: 0.6428225040435791\n",
      "Epoch: 31, Loss: 0.5549046397209167\n",
      "Epoch: 31, Loss: 0.6200526356697083\n",
      "Epoch: 31, Loss: 0.5675504803657532\n",
      "Epoch: 31, Loss: 0.7159863114356995\n",
      "Epoch: 31, Loss: 0.8500882387161255\n",
      "Epoch: 32, Loss: 0.8233594298362732\n",
      "Epoch: 32, Loss: 0.6429818272590637\n",
      "Epoch: 32, Loss: 0.8149198293685913\n",
      "Epoch: 32, Loss: 0.6630842089653015\n",
      "Epoch: 32, Loss: 0.6349425911903381\n",
      "Epoch: 33, Loss: 0.9302794933319092\n",
      "Epoch: 33, Loss: 0.6135872602462769\n",
      "Epoch: 33, Loss: 0.6085202097892761\n",
      "Epoch: 33, Loss: 0.7160044312477112\n",
      "Epoch: 33, Loss: 0.6456331610679626\n",
      "Epoch: 34, Loss: 0.5128102898597717\n",
      "Epoch: 34, Loss: 0.6373575329780579\n",
      "Epoch: 34, Loss: 0.6973585486412048\n",
      "Epoch: 34, Loss: 0.6617299318313599\n",
      "Epoch: 34, Loss: 0.6507983207702637\n",
      "Epoch: 35, Loss: 0.5687659978866577\n",
      "Epoch: 35, Loss: 0.6583108901977539\n",
      "Epoch: 35, Loss: 0.6880593299865723\n",
      "Epoch: 35, Loss: 0.651183009147644\n",
      "Epoch: 35, Loss: 0.6416935920715332\n",
      "Epoch: 36, Loss: 0.7186745405197144\n",
      "Epoch: 36, Loss: 0.7223417162895203\n",
      "Epoch: 36, Loss: 0.7019262909889221\n",
      "Epoch: 36, Loss: 0.6975158452987671\n",
      "Epoch: 36, Loss: 0.8150665760040283\n",
      "Epoch: 37, Loss: 0.9751184582710266\n",
      "Epoch: 37, Loss: 0.6834266185760498\n",
      "Epoch: 37, Loss: 0.49520981311798096\n",
      "Epoch: 37, Loss: 0.9635816216468811\n",
      "Epoch: 37, Loss: 0.6785781383514404\n",
      "Epoch: 38, Loss: 0.5414950847625732\n",
      "Epoch: 38, Loss: 0.7637686729431152\n",
      "Epoch: 38, Loss: 0.6810057163238525\n",
      "Epoch: 38, Loss: 0.7611373662948608\n",
      "Epoch: 38, Loss: 0.6807681918144226\n",
      "Epoch: 39, Loss: 0.6955721974372864\n",
      "Epoch: 39, Loss: 0.6464629173278809\n",
      "Epoch: 39, Loss: 0.69366455078125\n",
      "Epoch: 39, Loss: 0.5803235769271851\n",
      "Epoch: 39, Loss: 0.6534098386764526\n",
      "Epoch: 40, Loss: 0.4728122353553772\n",
      "Epoch: 40, Loss: 0.8975386023521423\n",
      "Epoch: 40, Loss: 0.9180386066436768\n",
      "Epoch: 40, Loss: 0.9283563494682312\n",
      "Epoch: 40, Loss: 0.7061617970466614\n",
      "Epoch: 41, Loss: 0.635073184967041\n",
      "Epoch: 41, Loss: 0.8094326853752136\n",
      "Epoch: 41, Loss: 0.7366654276847839\n",
      "Epoch: 41, Loss: 0.6633962988853455\n",
      "Epoch: 41, Loss: 0.5836498141288757\n",
      "Epoch: 42, Loss: 0.5882322192192078\n",
      "Epoch: 42, Loss: 0.9721988439559937\n",
      "Epoch: 42, Loss: 0.7424160242080688\n",
      "Epoch: 42, Loss: 0.6838322281837463\n",
      "Epoch: 42, Loss: 0.6394661664962769\n",
      "Epoch: 43, Loss: 0.6936273574829102\n",
      "Epoch: 43, Loss: 0.6504921913146973\n",
      "Epoch: 43, Loss: 0.5726158022880554\n",
      "Epoch: 43, Loss: 0.718974232673645\n",
      "Epoch: 43, Loss: 0.5027174353599548\n",
      "Epoch: 44, Loss: 0.6321418285369873\n",
      "Epoch: 44, Loss: 0.623778760433197\n",
      "Epoch: 44, Loss: 0.6450822353363037\n",
      "Epoch: 44, Loss: 0.6550472974777222\n",
      "Epoch: 44, Loss: 0.7490686774253845\n",
      "Epoch: 45, Loss: 0.7038171291351318\n",
      "Epoch: 45, Loss: 0.6885856986045837\n",
      "Epoch: 45, Loss: 0.65860915184021\n",
      "Epoch: 45, Loss: 0.5100576281547546\n",
      "Epoch: 45, Loss: 0.7633836269378662\n",
      "Epoch: 46, Loss: 0.6725361943244934\n",
      "Epoch: 46, Loss: 0.5873651504516602\n",
      "Epoch: 46, Loss: 0.7668316960334778\n",
      "Epoch: 46, Loss: 0.6283909678459167\n",
      "Epoch: 46, Loss: 0.7502889633178711\n",
      "Epoch: 47, Loss: 0.6542621850967407\n",
      "Epoch: 47, Loss: 0.6038209795951843\n",
      "Epoch: 47, Loss: 0.8172863721847534\n",
      "Epoch: 47, Loss: 0.5683843493461609\n",
      "Epoch: 47, Loss: 0.6172319054603577\n",
      "Epoch: 48, Loss: 0.7175834774971008\n",
      "Epoch: 48, Loss: 0.4197646975517273\n",
      "Epoch: 48, Loss: 0.6799317002296448\n",
      "Epoch: 48, Loss: 0.3861521780490875\n",
      "Epoch: 48, Loss: 0.8221048712730408\n",
      "Epoch: 49, Loss: 0.7149546146392822\n",
      "Epoch: 49, Loss: 0.6262046098709106\n",
      "Epoch: 49, Loss: 0.6533530950546265\n",
      "Epoch: 49, Loss: 0.6592798829078674\n",
      "Epoch: 49, Loss: 0.6667527556419373\n",
      "Epoch: 50, Loss: 0.729738712310791\n",
      "Epoch: 50, Loss: 0.5808578729629517\n",
      "Epoch: 50, Loss: 0.6650559306144714\n",
      "Epoch: 50, Loss: 0.7057734727859497\n",
      "Epoch: 50, Loss: 0.7173115611076355\n",
      "Epoch: 51, Loss: 0.6419289112091064\n",
      "Epoch: 51, Loss: 0.6428823471069336\n",
      "Epoch: 51, Loss: 0.7122936248779297\n",
      "Epoch: 51, Loss: 0.6658684015274048\n",
      "Epoch: 51, Loss: 0.6505870819091797\n",
      "Epoch: 52, Loss: 0.6456302404403687\n",
      "Epoch: 52, Loss: 0.6635639071464539\n",
      "Epoch: 52, Loss: 0.7116765975952148\n",
      "Epoch: 52, Loss: 0.6164427995681763\n",
      "Epoch: 52, Loss: 0.5942463874816895\n",
      "Epoch: 53, Loss: 0.5877429246902466\n",
      "Epoch: 53, Loss: 0.6023158431053162\n",
      "Epoch: 53, Loss: 0.5861071348190308\n",
      "Epoch: 53, Loss: 0.8897528648376465\n",
      "Epoch: 53, Loss: 0.6553202867507935\n",
      "Epoch: 54, Loss: 0.651258647441864\n",
      "Epoch: 54, Loss: 0.6469444036483765\n",
      "Epoch: 54, Loss: 0.6987981200218201\n",
      "Epoch: 54, Loss: 0.6650360226631165\n",
      "Epoch: 54, Loss: 0.7239043116569519\n",
      "Epoch: 55, Loss: 0.7036462426185608\n",
      "Epoch: 55, Loss: 0.6588000655174255\n",
      "Epoch: 55, Loss: 0.676738977432251\n",
      "Epoch: 55, Loss: 0.680579423904419\n",
      "Epoch: 55, Loss: 0.6438772082328796\n",
      "Epoch: 56, Loss: 0.6554474234580994\n",
      "Epoch: 56, Loss: 0.6881661415100098\n",
      "Epoch: 56, Loss: 0.7762693166732788\n",
      "Epoch: 56, Loss: 0.6544280648231506\n",
      "Epoch: 56, Loss: 0.8090468645095825\n",
      "Epoch: 57, Loss: 0.620751678943634\n",
      "Epoch: 57, Loss: 0.603246808052063\n",
      "Epoch: 57, Loss: 0.5866740345954895\n",
      "Epoch: 57, Loss: 0.6150702238082886\n",
      "Epoch: 57, Loss: 0.7406339645385742\n",
      "Epoch: 58, Loss: 0.6451915502548218\n",
      "Epoch: 58, Loss: 0.6582024097442627\n",
      "Epoch: 58, Loss: 0.7026001811027527\n",
      "Epoch: 58, Loss: 0.7556042671203613\n",
      "Epoch: 58, Loss: 0.7015146017074585\n",
      "Epoch: 59, Loss: 0.7022210955619812\n",
      "Epoch: 59, Loss: 0.6660109162330627\n",
      "Epoch: 59, Loss: 0.6886395812034607\n",
      "Epoch: 59, Loss: 0.7243441343307495\n",
      "Epoch: 59, Loss: 0.7151970267295837\n",
      "Epoch: 60, Loss: 0.6706939339637756\n",
      "Epoch: 60, Loss: 0.6859004497528076\n",
      "Epoch: 60, Loss: 0.6805671453475952\n",
      "Epoch: 60, Loss: 0.6789104342460632\n",
      "Epoch: 60, Loss: 0.685250461101532\n",
      "Epoch: 61, Loss: 0.6767206192016602\n",
      "Epoch: 61, Loss: 0.703849732875824\n",
      "Epoch: 61, Loss: 0.6494866013526917\n",
      "Epoch: 61, Loss: 0.7273252010345459\n",
      "Epoch: 61, Loss: 0.7209374308586121\n",
      "Epoch: 62, Loss: 0.6093714833259583\n",
      "Epoch: 62, Loss: 0.7250702381134033\n",
      "Epoch: 62, Loss: 0.6580243110656738\n",
      "Epoch: 62, Loss: 0.7122077941894531\n",
      "Epoch: 62, Loss: 0.7133916020393372\n",
      "Epoch: 63, Loss: 0.6052542924880981\n",
      "Epoch: 63, Loss: 0.7149864435195923\n",
      "Epoch: 63, Loss: 0.6980518698692322\n",
      "Epoch: 63, Loss: 0.7345464825630188\n",
      "Epoch: 63, Loss: 0.7024824023246765\n",
      "Epoch: 64, Loss: 0.7174973487854004\n",
      "Epoch: 64, Loss: 0.7081784009933472\n",
      "Epoch: 64, Loss: 0.6612468957901001\n",
      "Epoch: 64, Loss: 0.7261605858802795\n",
      "Epoch: 64, Loss: 0.6514080762863159\n",
      "Epoch: 65, Loss: 0.6560389995574951\n",
      "Epoch: 65, Loss: 0.6577759385108948\n",
      "Epoch: 65, Loss: 0.7228248119354248\n",
      "Epoch: 65, Loss: 0.6380690336227417\n",
      "Epoch: 65, Loss: 0.6471000909805298\n",
      "Epoch: 66, Loss: 0.6362757086753845\n",
      "Epoch: 66, Loss: 0.6456321477890015\n",
      "Epoch: 66, Loss: 0.799657940864563\n",
      "Epoch: 66, Loss: 0.6737870573997498\n",
      "Epoch: 66, Loss: 0.6189954280853271\n",
      "Epoch: 67, Loss: 0.6715947389602661\n",
      "Epoch: 67, Loss: 0.6360133290290833\n",
      "Epoch: 67, Loss: 0.6839055418968201\n",
      "Epoch: 67, Loss: 0.723023533821106\n",
      "Epoch: 67, Loss: 0.6924280524253845\n",
      "Epoch: 68, Loss: 0.7502849102020264\n",
      "Epoch: 68, Loss: 0.7205880284309387\n",
      "Epoch: 68, Loss: 0.678423285484314\n",
      "Epoch: 68, Loss: 0.6036890745162964\n",
      "Epoch: 68, Loss: 0.6091090440750122\n",
      "Epoch: 69, Loss: 0.6609497666358948\n",
      "Epoch: 69, Loss: 0.6599965691566467\n",
      "Epoch: 69, Loss: 0.7151461839675903\n",
      "Epoch: 69, Loss: 0.6443405747413635\n",
      "Epoch: 69, Loss: 0.6669899821281433\n",
      "Epoch: 70, Loss: 0.6916359066963196\n",
      "Epoch: 70, Loss: 0.6418933272361755\n",
      "Epoch: 70, Loss: 0.7007014155387878\n",
      "Epoch: 70, Loss: 0.6432117223739624\n",
      "Epoch: 70, Loss: 0.6486341953277588\n",
      "Epoch: 71, Loss: 0.6351521611213684\n",
      "Epoch: 71, Loss: 0.7131139039993286\n",
      "Epoch: 71, Loss: 0.6834771037101746\n",
      "Epoch: 71, Loss: 0.7152464985847473\n",
      "Epoch: 71, Loss: 0.7456330060958862\n",
      "Epoch: 72, Loss: 0.892622172832489\n",
      "Epoch: 72, Loss: 0.7276312112808228\n",
      "Epoch: 72, Loss: 0.8512699007987976\n",
      "Epoch: 72, Loss: 0.694449782371521\n",
      "Epoch: 72, Loss: 0.6729496717453003\n",
      "Epoch: 73, Loss: 0.6943172812461853\n",
      "Epoch: 73, Loss: 0.6677762269973755\n",
      "Epoch: 73, Loss: 0.753173828125\n",
      "Epoch: 73, Loss: 0.5035201907157898\n",
      "Epoch: 73, Loss: 0.6516427397727966\n",
      "Epoch: 74, Loss: 0.766507089138031\n",
      "Epoch: 74, Loss: 0.6419959664344788\n",
      "Epoch: 74, Loss: 0.7096143960952759\n",
      "Epoch: 74, Loss: 0.670935332775116\n",
      "Epoch: 74, Loss: 0.7748783826828003\n",
      "Epoch: 75, Loss: 0.7281515598297119\n",
      "Epoch: 75, Loss: 0.6934402585029602\n",
      "Epoch: 75, Loss: 0.6981772184371948\n",
      "Epoch: 75, Loss: 0.7443050146102905\n",
      "Epoch: 75, Loss: 0.8117772936820984\n",
      "Epoch: 76, Loss: 0.712678849697113\n",
      "Epoch: 76, Loss: 0.6479902863502502\n",
      "Epoch: 76, Loss: 0.7166696190834045\n",
      "Epoch: 76, Loss: 0.714723527431488\n",
      "Epoch: 76, Loss: 0.6875931024551392\n",
      "Epoch: 77, Loss: 0.7023985385894775\n",
      "Epoch: 77, Loss: 0.6928598880767822\n",
      "Epoch: 77, Loss: 0.6375978589057922\n",
      "Epoch: 77, Loss: 0.7346798181533813\n",
      "Epoch: 77, Loss: 0.6874108910560608\n",
      "Epoch: 78, Loss: 0.6692668199539185\n",
      "Epoch: 78, Loss: 0.7672961950302124\n",
      "Epoch: 78, Loss: 0.6878975033760071\n",
      "Epoch: 78, Loss: 0.7444371581077576\n",
      "Epoch: 78, Loss: 0.6181685328483582\n",
      "Epoch: 79, Loss: 0.7408337593078613\n",
      "Epoch: 79, Loss: 0.6956928968429565\n",
      "Epoch: 79, Loss: 0.6794587969779968\n",
      "Epoch: 79, Loss: 0.6941348314285278\n",
      "Epoch: 79, Loss: 0.7083191275596619\n",
      "Epoch: 80, Loss: 0.675403356552124\n",
      "Epoch: 80, Loss: 0.6977092027664185\n",
      "Epoch: 80, Loss: 0.7917901277542114\n",
      "Epoch: 80, Loss: 0.6562392115592957\n",
      "Epoch: 80, Loss: 0.6017573475837708\n",
      "Epoch: 81, Loss: 0.6546838879585266\n",
      "Epoch: 81, Loss: 0.712827205657959\n",
      "Epoch: 81, Loss: 0.7104406356811523\n",
      "Epoch: 81, Loss: 0.7590279579162598\n",
      "Epoch: 81, Loss: 0.6770333051681519\n",
      "Epoch: 82, Loss: 0.699205219745636\n",
      "Epoch: 82, Loss: 0.66700279712677\n",
      "Epoch: 82, Loss: 0.7019460201263428\n",
      "Epoch: 82, Loss: 0.7202727198600769\n",
      "Epoch: 82, Loss: 0.6549201011657715\n",
      "Epoch: 83, Loss: 0.704325795173645\n",
      "Epoch: 83, Loss: 0.6788887977600098\n",
      "Epoch: 83, Loss: 0.6817353367805481\n",
      "Epoch: 83, Loss: 0.6826661229133606\n",
      "Epoch: 83, Loss: 0.6841928362846375\n",
      "Epoch: 84, Loss: 0.6869033575057983\n",
      "Epoch: 84, Loss: 0.6634677052497864\n",
      "Epoch: 84, Loss: 0.7342970371246338\n",
      "Epoch: 84, Loss: 0.706876277923584\n",
      "Epoch: 84, Loss: 0.6670278310775757\n",
      "Epoch: 85, Loss: 0.667635977268219\n",
      "Epoch: 85, Loss: 0.6807346343994141\n",
      "Epoch: 85, Loss: 0.6761147975921631\n",
      "Epoch: 85, Loss: 0.6954495906829834\n",
      "Epoch: 85, Loss: 0.7266585826873779\n",
      "Epoch: 86, Loss: 0.7181125283241272\n",
      "Epoch: 86, Loss: 0.618821382522583\n",
      "Epoch: 86, Loss: 0.743022084236145\n",
      "Epoch: 86, Loss: 0.620104968547821\n",
      "Epoch: 86, Loss: 0.6915913224220276\n",
      "Epoch: 87, Loss: 0.6599992513656616\n",
      "Epoch: 87, Loss: 0.6762290000915527\n",
      "Epoch: 87, Loss: 0.6547744274139404\n",
      "Epoch: 87, Loss: 0.6890526413917542\n",
      "Epoch: 87, Loss: 0.7239493131637573\n",
      "Epoch: 88, Loss: 0.727663516998291\n",
      "Epoch: 88, Loss: 0.7597970962524414\n",
      "Epoch: 88, Loss: 0.8690525889396667\n",
      "Epoch: 88, Loss: 0.7404127717018127\n",
      "Epoch: 88, Loss: 0.7781761884689331\n",
      "Epoch: 89, Loss: 0.622517466545105\n",
      "Epoch: 89, Loss: 0.7207076549530029\n",
      "Epoch: 89, Loss: 0.7059274911880493\n",
      "Epoch: 89, Loss: 0.65891033411026\n",
      "Epoch: 89, Loss: 0.6905162930488586\n",
      "Epoch: 90, Loss: 0.75055330991745\n",
      "Epoch: 90, Loss: 0.6694977879524231\n",
      "Epoch: 90, Loss: 0.6790241003036499\n",
      "Epoch: 90, Loss: 0.6991438269615173\n",
      "Epoch: 90, Loss: 0.6612767577171326\n",
      "Epoch: 91, Loss: 0.7076316475868225\n",
      "Epoch: 91, Loss: 0.6330773234367371\n",
      "Epoch: 91, Loss: 0.7177171111106873\n",
      "Epoch: 91, Loss: 0.48804333806037903\n",
      "Epoch: 91, Loss: 0.729766309261322\n",
      "Epoch: 92, Loss: 0.6925980448722839\n",
      "Epoch: 92, Loss: 0.6383485794067383\n",
      "Epoch: 92, Loss: 0.7616896033287048\n",
      "Epoch: 92, Loss: 0.6165331602096558\n",
      "Epoch: 92, Loss: 0.5458290576934814\n",
      "Epoch: 93, Loss: 0.7564308047294617\n",
      "Epoch: 93, Loss: 0.7296069264411926\n",
      "Epoch: 93, Loss: 0.7847263813018799\n",
      "Epoch: 93, Loss: 0.7191494107246399\n",
      "Epoch: 93, Loss: 0.6569310426712036\n",
      "Epoch: 94, Loss: 0.6280453205108643\n",
      "Epoch: 94, Loss: 0.7181465029716492\n",
      "Epoch: 94, Loss: 0.7394944429397583\n",
      "Epoch: 94, Loss: 0.8791165351867676\n",
      "Epoch: 94, Loss: 0.6505561470985413\n",
      "Epoch: 95, Loss: 0.6551020741462708\n",
      "Epoch: 95, Loss: 0.7505677938461304\n",
      "Epoch: 95, Loss: 0.5792012214660645\n",
      "Epoch: 95, Loss: 0.7621002793312073\n",
      "Epoch: 95, Loss: 0.5807545185089111\n",
      "Epoch: 96, Loss: 0.6607707738876343\n",
      "Epoch: 96, Loss: 0.6513065099716187\n",
      "Epoch: 96, Loss: 0.6002456545829773\n",
      "Epoch: 96, Loss: 0.7325619459152222\n",
      "Epoch: 96, Loss: 0.6628934144973755\n",
      "Epoch: 97, Loss: 0.5727461576461792\n",
      "Epoch: 97, Loss: 0.5736093521118164\n",
      "Epoch: 97, Loss: 0.6637737154960632\n",
      "Epoch: 97, Loss: 0.5822138786315918\n",
      "Epoch: 97, Loss: 0.6757315397262573\n",
      "Epoch: 98, Loss: 0.6995119452476501\n",
      "Epoch: 98, Loss: 0.6555672287940979\n",
      "Epoch: 98, Loss: 0.7032018303871155\n",
      "Epoch: 98, Loss: 0.6604166626930237\n",
      "Epoch: 98, Loss: 0.7074623703956604\n",
      "Epoch: 99, Loss: 0.6496420502662659\n",
      "Epoch: 99, Loss: 0.6467350125312805\n",
      "Epoch: 99, Loss: 0.7539981007575989\n",
      "Epoch: 99, Loss: 0.7662659287452698\n",
      "Epoch: 99, Loss: 0.679375946521759\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "i=0\n",
    "for epoch in range(epochs):\n",
    "    for sentence, label in train_data:\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        embedded_sentences, word_dict_ = process_sentences(sentence, embedding_dim, max_len)\n",
    "        # print(f'Embedded sentences: {embedded_sentences.shape}')\n",
    "        output = model(embedded_sentences)\n",
    "        # print(type(output), output)\n",
    "        # print(type(label), label)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        if i%10==0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "with torch.no_grad():\n",
    "    for sentence in test_sentences:\n",
    "        embedded_sentence, _ = process_sentences(sentence, d_word, max_len)\n",
    "        output = model(embedded_sentence)\n",
    "        prediction = 1 if output.item() > 0.5 else 0\n",
    "        test_results.append((sentence, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'This is fantastic' -> Predicted Sentiment: 0\n",
      "Sentence: 'I regret buying this' -> Predicted Sentiment: 0\n",
      "Sentence: 'I absolutely love it' -> Predicted Sentiment: 0\n",
      "Sentence: 'Worst experience ever' -> Predicted Sentiment: 1\n",
      "Sentence: 'Best purchase of my life' -> Predicted Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "for sentence, pred in test_results:\n",
    "    print(f\"Sentence: '{sentence}' -> Predicted Sentiment: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
