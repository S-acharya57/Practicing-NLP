{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRzKD9hdTVZ8"
      },
      "source": [
        "## Kantai Bert with Byte Level Byte pair Encoding Tokenizer\n",
        "\n",
        "1. Obtaining the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2fNX9iMTc85"
      },
      "source": [
        "### 1. Obtaining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGIfBaH2TJzb",
        "outputId": "0696feac-a73b-4804-c8d6-44e3dc181981"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 10.7M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  3 10.7M    3  336k    0     0   195k      0  0:00:56  0:00:01  0:00:55  195k\n",
            " 41 10.7M   41 4591k    0     0  1651k      0  0:00:06  0:00:02  0:00:04 1653k\n",
            "100 10.7M  100 10.7M    0     0  3811k      0  0:00:02  0:00:02 --:--:-- 3816k\n"
          ]
        }
      ],
      "source": [
        "# dataset of books by Immanuel Kant\n",
        "\n",
        "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DxY3CCOETT4y",
        "outputId": "e719381c-3411-4059-e2bb-328b6e2f0516"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye7esR-kUDD-",
        "outputId": "9391e176-62ba-4f5b-cbcb-6aa61e60f84e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'grep' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# getting versions of transformers and tokenizers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "THHowdEnVRXQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\After\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tokenizers\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZrKCSNtUWT3",
        "outputId": "8f668317-5ab0-4ef3-af3e-ca76df2647b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['kant.txt']\n",
            "Tokenizer: Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
            "CPU times: total: 13.5 s\n",
            "Wall time: 1.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# using bytelevelbpetokenizer\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"*.txt\")]\n",
        "print(paths)\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
        "print(f'Tokenizer: {tokenizer}')\n",
        "\n",
        "tokenizer.train(files=paths, vocab_size=50000, min_frequency=2, special_tokens=[\n",
        "    \"<start>\",\n",
        "    \"<pad>\",\n",
        "    \"<\\s>\",\n",
        "    \"<unknown>\",\n",
        "    \"<mask>\",\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ15RtZXVu-d",
        "outputId": "47fa0da3-17fd-4e81-cc00-29c40f87c2ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tokenizer\\\\vocab.json', 'Tokenizer\\\\merges.txt']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "tokenizer_dir = 'Tokenizer'\n",
        "\n",
        "# print(os.path.exists(tokenizer_dir))\n",
        "if os.path.exists(tokenizer_dir):\n",
        "    # print(os.path.listdir)\n",
        "    pass\n",
        "\n",
        "if not os.path.exists(tokenizer_dir):\n",
        "    os.mkdir(tokenizer_dir)\n",
        "    print(\"Dir made\")\n",
        "tokenizer.save_model('Tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0fbUlN-VV6NX"
      },
      "outputs": [],
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "# from tokenizers.processors import BertProcesssing \n",
        "import tokenizers.processors \n",
        "from tokenizers.processors import BertProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=19296, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"Tokenizer/vocab.json\",\n",
        "    \"Tokenizer/merges.txt\"\n",
        ")\n",
        "tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding: Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens are : ['The', 'Ġthinking', 'Ġof', 'Ġthe', 'Ġway', 'Ġof', 'Ġlife']\n"
          ]
        }
      ],
      "source": [
        "# encoding a sentence\n",
        "encoding = tokenizer.encode(\"The thinking of the way of life\")\n",
        "print(f\"Encoding: {encoding}\")\n",
        "print(f\"Tokens are : {encoding.tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tokenizers.processors.ByteLevel at 0x198c57bbfc0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer._tokenizer.post_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"<s>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fitting the BERT model \n",
        "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "#     (\"\", tokenizer.token_to_id(\"\")),\n",
        "#     (\"\", tokenizer.token_to_id(\"\")),\n",
        "# )\n",
        "tokenizer.enable_truncation(max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding: Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens are : ['The', 'Ġthinking', 'Ġof', 'Ġthe', 'Ġway', 'Ġof', 'Ġlife', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"The thinking of the way of life.\")\n",
        "print(f\"Encoding: {encoding}\")\n",
        "print(f\"Tokens are : {encoding.tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch \n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.42.3\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 52000\n",
              "}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size = 52000,\n",
        "    max_position_embeddings = 514,\n",
        "    num_attention_heads = 12,\n",
        "    num_hidden_layers = 6,\n",
        "    type_vocab_size = 1,\n",
        ")\n",
        "config "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='tokenizer', vocab_size=19296, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "\t19296: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19297: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19298: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reloading tokenizer of Roberta\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\n",
        "    \"tokenizer\",\n",
        "    max_length=512,\n",
        ")\n",
        "tokenizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<generator object Module.parameters at 0x00000198D04FD700>, 83504416)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.parameters(), model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106\n",
            "tensor([0.0012, 0.0027, 0.0071, 0.0134, 0.0075], grad_fn=<SliceBackward0>) torch.Size([52000, 768])\n"
          ]
        }
      ],
      "source": [
        "params = list(model.parameters())\n",
        "print(len(params))\n",
        "print(params[0][0, :5], params[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 52000 768 39936000\n",
            "1 514 768 394752\n",
            "2 1 768 768\n",
            "3 768 768\n",
            "4 768 768\n",
            "5 768 768 589824\n",
            "6 768 768\n",
            "7 768 768 589824\n",
            "8 768 768\n",
            "9 768 768 589824\n",
            "10 768 768\n",
            "11 768 768 589824\n",
            "12 768 768\n",
            "13 768 768\n",
            "14 768 768\n",
            "15 3072 768 2359296\n",
            "16 3072 3072\n",
            "17 768 3072 2359296\n",
            "18 768 768\n",
            "19 768 768\n",
            "20 768 768\n",
            "21 768 768 589824\n",
            "22 768 768\n",
            "23 768 768 589824\n",
            "24 768 768\n",
            "25 768 768 589824\n",
            "26 768 768\n",
            "27 768 768 589824\n",
            "28 768 768\n",
            "29 768 768\n",
            "30 768 768\n",
            "31 3072 768 2359296\n",
            "32 3072 3072\n",
            "33 768 3072 2359296\n",
            "34 768 768\n",
            "35 768 768\n",
            "36 768 768\n",
            "37 768 768 589824\n",
            "38 768 768\n",
            "39 768 768 589824\n",
            "40 768 768\n",
            "41 768 768 589824\n",
            "42 768 768\n",
            "43 768 768 589824\n",
            "44 768 768\n",
            "45 768 768\n",
            "46 768 768\n",
            "47 3072 768 2359296\n",
            "48 3072 3072\n",
            "49 768 3072 2359296\n",
            "50 768 768\n",
            "51 768 768\n",
            "52 768 768\n",
            "53 768 768 589824\n",
            "54 768 768\n",
            "55 768 768 589824\n",
            "56 768 768\n",
            "57 768 768 589824\n",
            "58 768 768\n",
            "59 768 768 589824\n",
            "60 768 768\n",
            "61 768 768\n",
            "62 768 768\n",
            "63 3072 768 2359296\n",
            "64 3072 3072\n",
            "65 768 3072 2359296\n",
            "66 768 768\n",
            "67 768 768\n",
            "68 768 768\n",
            "69 768 768 589824\n",
            "70 768 768\n",
            "71 768 768 589824\n",
            "72 768 768\n",
            "73 768 768 589824\n",
            "74 768 768\n",
            "75 768 768 589824\n",
            "76 768 768\n",
            "77 768 768\n",
            "78 768 768\n",
            "79 3072 768 2359296\n",
            "80 3072 3072\n",
            "81 768 3072 2359296\n",
            "82 768 768\n",
            "83 768 768\n",
            "84 768 768\n",
            "85 768 768 589824\n",
            "86 768 768\n",
            "87 768 768 589824\n",
            "88 768 768\n",
            "89 768 768 589824\n",
            "90 768 768\n",
            "91 768 768 589824\n",
            "92 768 768\n",
            "93 768 768\n",
            "94 768 768\n",
            "95 3072 768 2359296\n",
            "96 3072 3072\n",
            "97 768 3072 2359296\n",
            "98 768 768\n",
            "99 768 768\n",
            "100 768 768\n",
            "101 52000 52000\n",
            "102 768 768 589824\n",
            "103 768 768\n",
            "104 768 768\n",
            "105 768 768\n",
            "83504416\n"
          ]
        }
      ],
      "source": [
        "# counting the parameters\n",
        "np=0\n",
        "for p in range(0,len(params)):#number of tensors\n",
        "    PL2=True\n",
        "    try:\n",
        "        L2=len(params[p][0]) #check if 2D\n",
        "    except:\n",
        "        L2=1             #not 2D but 1D\n",
        "        PL2=False\n",
        "    L1=len(params[p])      \n",
        "    L3=L1*L2\n",
        "    np+=L3             # number of parameters per tensor\n",
        "    if PL2==True:\n",
        "        print(p,L1,L2,L3)  # displaying the sizes of the parameters\n",
        "    if PL2==False:\n",
        "        print(p,L1,L3)  # displaying the sizes of the parameters\n",
        "\n",
        "print(np)              # total number of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\After\\nlp\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 23 s\n",
            "Wall time: 22.9 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x198d010f650>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"kant.txt\",\n",
        "    block_size = 128,\n",
        ")\n",
        "dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataCollatorForLanguageModeling(tokenizer=RobertaTokenizer(name_or_path='tokenizer', vocab_size=19296, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "\t19296: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19297: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19298: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}, mlm=True, mlm_probability=0.14, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data collator that takes sample from dataset and collates to batches\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# mlm probability: % of masked tokens in training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.14,\n",
        ")\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<transformers.trainer.Trainer at 0x198f012c110>,\n",
              " TrainingArguments(\n",
              " _n_gpu=1,\n",
              " accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              " adafactor=False,\n",
              " adam_beta1=0.9,\n",
              " adam_beta2=0.999,\n",
              " adam_epsilon=1e-08,\n",
              " auto_find_batch_size=False,\n",
              " batch_eval_metrics=False,\n",
              " bf16=False,\n",
              " bf16_full_eval=False,\n",
              " data_seed=None,\n",
              " dataloader_drop_last=False,\n",
              " dataloader_num_workers=0,\n",
              " dataloader_persistent_workers=False,\n",
              " dataloader_pin_memory=True,\n",
              " dataloader_prefetch_factor=None,\n",
              " ddp_backend=None,\n",
              " ddp_broadcast_buffers=None,\n",
              " ddp_bucket_cap_mb=None,\n",
              " ddp_find_unused_parameters=None,\n",
              " ddp_timeout=1800,\n",
              " debug=[],\n",
              " deepspeed=None,\n",
              " disable_tqdm=False,\n",
              " dispatch_batches=None,\n",
              " do_eval=False,\n",
              " do_predict=False,\n",
              " do_train=False,\n",
              " eval_accumulation_steps=None,\n",
              " eval_delay=0,\n",
              " eval_do_concat_batches=True,\n",
              " eval_on_start=False,\n",
              " eval_steps=None,\n",
              " eval_strategy=IntervalStrategy.NO,\n",
              " evaluation_strategy=None,\n",
              " fp16=False,\n",
              " fp16_backend=auto,\n",
              " fp16_full_eval=False,\n",
              " fp16_opt_level=O1,\n",
              " fsdp=[],\n",
              " fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              " fsdp_min_num_params=0,\n",
              " fsdp_transformer_layer_cls_to_wrap=None,\n",
              " full_determinism=False,\n",
              " gradient_accumulation_steps=1,\n",
              " gradient_checkpointing=False,\n",
              " gradient_checkpointing_kwargs=None,\n",
              " greater_is_better=None,\n",
              " group_by_length=False,\n",
              " half_precision_backend=auto,\n",
              " hub_always_push=False,\n",
              " hub_model_id=None,\n",
              " hub_private_repo=False,\n",
              " hub_strategy=HubStrategy.EVERY_SAVE,\n",
              " hub_token=<HUB_TOKEN>,\n",
              " ignore_data_skip=False,\n",
              " include_inputs_for_metrics=False,\n",
              " include_num_input_tokens_seen=False,\n",
              " include_tokens_per_second=False,\n",
              " jit_mode_eval=False,\n",
              " label_names=None,\n",
              " label_smoothing_factor=0.0,\n",
              " learning_rate=5e-05,\n",
              " length_column_name=length,\n",
              " load_best_model_at_end=False,\n",
              " local_rank=0,\n",
              " log_level=passive,\n",
              " log_level_replica=warning,\n",
              " log_on_each_node=True,\n",
              " logging_dir=/KantaiBERT\\runs\\Jul23_06-12-09_root,\n",
              " logging_first_step=False,\n",
              " logging_nan_inf_filter=True,\n",
              " logging_steps=500,\n",
              " logging_strategy=IntervalStrategy.STEPS,\n",
              " lr_scheduler_kwargs={},\n",
              " lr_scheduler_type=SchedulerType.LINEAR,\n",
              " max_grad_norm=1.0,\n",
              " max_steps=-1,\n",
              " metric_for_best_model=None,\n",
              " mp_parameters=,\n",
              " neftune_noise_alpha=None,\n",
              " no_cuda=False,\n",
              " num_train_epochs=1,\n",
              " optim=OptimizerNames.ADAMW_TORCH,\n",
              " optim_args=None,\n",
              " optim_target_modules=None,\n",
              " output_dir=/KantaiBERT,\n",
              " overwrite_output_dir=True,\n",
              " past_index=-1,\n",
              " per_device_eval_batch_size=8,\n",
              " per_device_train_batch_size=64,\n",
              " prediction_loss_only=False,\n",
              " push_to_hub=False,\n",
              " push_to_hub_model_id=None,\n",
              " push_to_hub_organization=None,\n",
              " push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              " ray_scope=last,\n",
              " remove_unused_columns=True,\n",
              " report_to=[],\n",
              " restore_callback_states_from_checkpoint=False,\n",
              " resume_from_checkpoint=None,\n",
              " run_name=/KantaiBERT,\n",
              " save_on_each_node=False,\n",
              " save_only_model=False,\n",
              " save_safetensors=True,\n",
              " save_steps=10000,\n",
              " save_strategy=IntervalStrategy.STEPS,\n",
              " save_total_limit=2,\n",
              " seed=42,\n",
              " skip_memory_metrics=True,\n",
              " split_batches=None,\n",
              " tf32=None,\n",
              " torch_compile=False,\n",
              " torch_compile_backend=None,\n",
              " torch_compile_mode=None,\n",
              " torchdynamo=None,\n",
              " tpu_metrics_debug=False,\n",
              " tpu_num_cores=None,\n",
              " use_cpu=False,\n",
              " use_ipex=False,\n",
              " use_legacy_prediction_loop=False,\n",
              " use_mps_device=False,\n",
              " warmup_ratio=0.0,\n",
              " warmup_steps=0,\n",
              " weight_decay=0.0,\n",
              " ))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"KantaiBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "trainer, training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 501/2672 [01:43<07:44,  4.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 6.6017, 'grad_norm': 3.233264923095703, 'learning_rate': 4.06437125748503e-05, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 1001/2672 [03:24<05:29,  5.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.7644, 'grad_norm': 4.23481559753418, 'learning_rate': 3.12874251497006e-05, 'epoch': 0.37}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 1501/2672 [05:07<03:53,  5.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.3077, 'grad_norm': 5.4278740882873535, 'learning_rate': 2.1931137724550898e-05, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▍  | 2001/2672 [06:51<02:14,  4.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.0592, 'grad_norm': 6.6849164962768555, 'learning_rate': 1.2574850299401197e-05, 'epoch': 0.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▎| 2500/2672 [08:33<00:34,  5.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.9503, 'grad_norm': 6.714309215545654, 'learning_rate': 3.218562874251497e-06, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2672/2672 [09:11<00:00,  4.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 551.3377, 'train_samples_per_second': 310.089, 'train_steps_per_second': 4.846, 'train_loss': 5.493957633743743, 'epoch': 1.0}\n",
            "CPU times: total: 9min 14s\n",
            "Wall time: 9min 12s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2672, training_loss=5.493957633743743, metrics={'train_runtime': 551.3377, 'train_samples_per_second': 310.089, 'train_steps_per_second': 4.846, 'total_flos': 873691623267840.0, 'train_loss': 5.493957633743743, 'epoch': 1.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"KantaiBERT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.7887444496154785,\n",
              "  'token': 18,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'Human thinking involves human.'},\n",
              " {'score': 0.00232885405421257,\n",
              "  'token': 67,\n",
              "  'token_str': '_',\n",
              "  'sequence': 'Human thinking involves human_'},\n",
              " {'score': 0.002071449998766184,\n",
              "  'token': 1031,\n",
              "  'token_str': ').',\n",
              "  'sequence': 'Human thinking involves human).'},\n",
              " {'score': 0.0019306758185848594,\n",
              "  'token': 429,\n",
              "  'token_str': '—',\n",
              "  'sequence': 'Human thinking involves human—'},\n",
              " {'score': 0.001685379189439118,\n",
              "  'token': 270,\n",
              "  'token_str': ' of',\n",
              "  'sequence': 'Human thinking involves human of'}]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fill_mask(\"Human thinking involves human <mask>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
