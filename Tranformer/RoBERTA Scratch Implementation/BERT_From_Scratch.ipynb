{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRzKD9hdTVZ8"
      },
      "source": [
        "## Kantai Bert with Byte Level Byte pair Encoding Tokenizer\n",
        "\n",
        "1. Obtaining the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2fNX9iMTc85"
      },
      "source": [
        "### 1. Obtaining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGIfBaH2TJzb",
        "outputId": "0d3b835c-8238-43fc-978f-bc214ef27ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.7M  100 10.7M    0     0  5067k      0  0:00:02  0:00:02 --:--:-- 5068k\n"
          ]
        }
      ],
      "source": [
        "# dataset of books by Immanuel Kant\n",
        "\n",
        "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "DxY3CCOETT4y"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye7esR-kUDD-",
        "outputId": "95fbf7d1-89ba-4c6b-eca5-96088cf1ae78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizers                       0.19.1\n",
            "transformers                     4.42.4\n"
          ]
        }
      ],
      "source": [
        "# getting versions of transformers and tokenizers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "THHowdEnVRXQ"
      },
      "outputs": [],
      "source": [
        "import tokenizers\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZrKCSNtUWT3",
        "outputId": "53ebfda9-83b4-44b5-8d26-d605eabcedb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kant.txt']\n",
            "Tokenizer: Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
            "CPU times: user 7.36 s, sys: 167 ms, total: 7.53 s\n",
            "Wall time: 9.05 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# using bytelevelbpetokenizer\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"*.txt\")]\n",
        "print(paths)\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
        "print(f'Tokenizer: {tokenizer}')\n",
        "\n",
        "tokenizer.train(files=paths, vocab_size=50000, min_frequency=2, special_tokens=[\n",
        "    \"<start>\",\n",
        "    \"<pad>\",\n",
        "    \"<\\s>\",\n",
        "    \"<unknown>\",\n",
        "    \"<mask>\",\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ15RtZXVu-d",
        "outputId": "c7826bfa-0eb6-49ab-8ae0-d0bf24794ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dir made\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenizer/vocab.json', 'Tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "tokenizer_dir = 'Tokenizer'\n",
        "\n",
        "# print(os.path.exists(tokenizer_dir))\n",
        "if os.path.exists(tokenizer_dir):\n",
        "    # print(os.path.listdir)\n",
        "    pass\n",
        "\n",
        "if not os.path.exists(tokenizer_dir):\n",
        "    os.mkdir(tokenizer_dir)\n",
        "    print(\"Dir made\")\n",
        "tokenizer.save_model('Tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSjpjY3HCgGY"
      },
      "source": [
        "## Loading Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0fbUlN-VV6NX"
      },
      "outputs": [],
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "# from tokenizers.processors import BertProcesssing\n",
        "import tokenizers.processors\n",
        "from tokenizers.processors import BertProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCHuL35rCgGZ",
        "outputId": "92ce6695-d3a7-4983-89a8-99ba957ce00f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=19296, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"Tokenizer/vocab.json\",\n",
        "    \"Tokenizer/merges.txt\"\n",
        ")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8wJzKYlCgGZ",
        "outputId": "42f6b53c-5056-4a5a-ab83-2760bd2a51c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens are : ['The', 'Ä thinking', 'Ä of', 'Ä the', 'Ä way', 'Ä of', 'Ä life']\n"
          ]
        }
      ],
      "source": [
        "# encoding a sentence\n",
        "encoding = tokenizer.encode(\"The thinking of the way of life\")\n",
        "print(f\"Encoding: {encoding}\")\n",
        "print(f\"Tokens are : {encoding.tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTgxvDa3CgGZ",
        "outputId": "320db7ee-a81a-4b55-a1c2-c73092da5480"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tokenizers.processors.ByteLevel at 0x7c024a8323d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenizer._tokenizer.post_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jnZVV0dsCgGZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"<s>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V6AcP6dWCgGZ"
      },
      "outputs": [],
      "source": [
        "# fitting the BERT model\n",
        "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "#     (\"\", tokenizer.token_to_id(\"\")),\n",
        "#     (\"\", tokenizer.token_to_id(\"\")),\n",
        "# )\n",
        "tokenizer.enable_truncation(max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR43AmZUCgGZ",
        "outputId": "0c9d37c3-bec5-4efa-d18f-ead0992d51c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens are : ['The', 'Ä thinking', 'Ä of', 'Ä the', 'Ä way', 'Ä of', 'Ä life', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"The thinking of the way of life.\")\n",
        "print(f\"Encoding: {encoding}\")\n",
        "print(f\"Tokens are : {encoding.tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcAWFy3rCgGa",
        "outputId": "21455289-d99b-479e-eba9-a48a5c003350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TiEp5mKCgGa"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER5Y8bnyCgGa",
        "outputId": "09a6d437-a0f8-4617-b353-1692bed0fed1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.42.4\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 52000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size = 52000,\n",
        "    max_position_embeddings = 514,\n",
        "    num_attention_heads = 12,\n",
        "    num_hidden_layers = 6,\n",
        "    type_vocab_size = 1,\n",
        ")\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQDUGaDXCgGa",
        "outputId": "eb826b48-c8b4-458a-a44c-10e07aa8f608"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='Tokenizer', vocab_size=19296, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "\t19296: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19297: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19298: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# reloading tokenizer of Roberta\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\n",
        "    \"Tokenizer\",\n",
        "    max_length=512,\n",
        ")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e8Cgpy6CgGa"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvX2GD_NCgGa",
        "outputId": "6541bc58-7b66-4f46-a41e-3ca9e8ce0471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82Lo9MGCgGa",
        "outputId": "c4363850-37be-4d55-d96e-211a6971b207"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<generator object Module.parameters at 0x7c0241c47d80>, 83504416)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.parameters(), model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPn64CGCCgGa",
        "outputId": "54876255-a1af-4f11-ead3-fc380eb5c1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106\n",
            "tensor([-0.0078, -0.0072,  0.0118,  0.0051,  0.0135], grad_fn=<SliceBackward0>) torch.Size([52000, 768])\n"
          ]
        }
      ],
      "source": [
        "params = list(model.parameters())\n",
        "print(len(params))\n",
        "print(params[0][0, :5], params[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxxWHB0qCgGa",
        "outputId": "e1ca1892-f77a-44de-8ee0-db1781ca5581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 52000 768 39936000\n",
            "1 514 768 394752\n",
            "2 1 768 768\n",
            "3 768 768\n",
            "4 768 768\n",
            "5 768 768 589824\n",
            "6 768 768\n",
            "7 768 768 589824\n",
            "8 768 768\n",
            "9 768 768 589824\n",
            "10 768 768\n",
            "11 768 768 589824\n",
            "12 768 768\n",
            "13 768 768\n",
            "14 768 768\n",
            "15 3072 768 2359296\n",
            "16 3072 3072\n",
            "17 768 3072 2359296\n",
            "18 768 768\n",
            "19 768 768\n",
            "20 768 768\n",
            "21 768 768 589824\n",
            "22 768 768\n",
            "23 768 768 589824\n",
            "24 768 768\n",
            "25 768 768 589824\n",
            "26 768 768\n",
            "27 768 768 589824\n",
            "28 768 768\n",
            "29 768 768\n",
            "30 768 768\n",
            "31 3072 768 2359296\n",
            "32 3072 3072\n",
            "33 768 3072 2359296\n",
            "34 768 768\n",
            "35 768 768\n",
            "36 768 768\n",
            "37 768 768 589824\n",
            "38 768 768\n",
            "39 768 768 589824\n",
            "40 768 768\n",
            "41 768 768 589824\n",
            "42 768 768\n",
            "43 768 768 589824\n",
            "44 768 768\n",
            "45 768 768\n",
            "46 768 768\n",
            "47 3072 768 2359296\n",
            "48 3072 3072\n",
            "49 768 3072 2359296\n",
            "50 768 768\n",
            "51 768 768\n",
            "52 768 768\n",
            "53 768 768 589824\n",
            "54 768 768\n",
            "55 768 768 589824\n",
            "56 768 768\n",
            "57 768 768 589824\n",
            "58 768 768\n",
            "59 768 768 589824\n",
            "60 768 768\n",
            "61 768 768\n",
            "62 768 768\n",
            "63 3072 768 2359296\n",
            "64 3072 3072\n",
            "65 768 3072 2359296\n",
            "66 768 768\n",
            "67 768 768\n",
            "68 768 768\n",
            "69 768 768 589824\n",
            "70 768 768\n",
            "71 768 768 589824\n",
            "72 768 768\n",
            "73 768 768 589824\n",
            "74 768 768\n",
            "75 768 768 589824\n",
            "76 768 768\n",
            "77 768 768\n",
            "78 768 768\n",
            "79 3072 768 2359296\n",
            "80 3072 3072\n",
            "81 768 3072 2359296\n",
            "82 768 768\n",
            "83 768 768\n",
            "84 768 768\n",
            "85 768 768 589824\n",
            "86 768 768\n",
            "87 768 768 589824\n",
            "88 768 768\n",
            "89 768 768 589824\n",
            "90 768 768\n",
            "91 768 768 589824\n",
            "92 768 768\n",
            "93 768 768\n",
            "94 768 768\n",
            "95 3072 768 2359296\n",
            "96 3072 3072\n",
            "97 768 3072 2359296\n",
            "98 768 768\n",
            "99 768 768\n",
            "100 768 768\n",
            "101 52000 52000\n",
            "102 768 768 589824\n",
            "103 768 768\n",
            "104 768 768\n",
            "105 768 768\n",
            "83504416\n"
          ]
        }
      ],
      "source": [
        "# counting the parameters\n",
        "np=0\n",
        "for p in range(0,len(params)):#number of tensors\n",
        "    PL2=True\n",
        "    try:\n",
        "        L2=len(params[p][0]) #check if 2D\n",
        "    except:\n",
        "        L2=1             #not 2D but 1D\n",
        "        PL2=False\n",
        "    L1=len(params[p])\n",
        "    L3=L1*L2\n",
        "    np+=L3             # number of parameters per tensor\n",
        "    if PL2==True:\n",
        "        print(p,L1,L2,L3)  # displaying the sizes of the parameters\n",
        "    if PL2==False:\n",
        "        print(p,L1,L3)  # displaying the sizes of the parameters\n",
        "\n",
        "print(np)              # total number of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJhgG7aCCgGb"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ZdT1SoCgGb",
        "outputId": "4c85c7ee-7f02-4320-cf51-1af269319552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.1 s, sys: 897 ms, total: 25 s\n",
            "Wall time: 25.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7c0241bb90f0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"kant.txt\",\n",
        "    block_size = 128,\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1XBnT1gCgGb",
        "outputId": "381791cd-49a6-4e7d-dcde-f007d45e82ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataCollatorForLanguageModeling(tokenizer=RobertaTokenizer(name_or_path='Tokenizer', vocab_size=19296, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "\t19296: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19297: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t19298: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}, mlm=True, mlm_probability=0.14, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# data collator that takes sample from dataset and collates to batches\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# mlm probability: % of masked tokens in training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.14,\n",
        ")\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNugmLZCgGb",
        "outputId": "d0b80356-5e6f-4048-f7f8-b44058ef7a60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<transformers.trainer.Trainer at 0x7c01ad1b8eb0>,\n",
              " TrainingArguments(\n",
              " _n_gpu=1,\n",
              " accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              " adafactor=False,\n",
              " adam_beta1=0.9,\n",
              " adam_beta2=0.999,\n",
              " adam_epsilon=1e-08,\n",
              " auto_find_batch_size=False,\n",
              " batch_eval_metrics=False,\n",
              " bf16=False,\n",
              " bf16_full_eval=False,\n",
              " data_seed=None,\n",
              " dataloader_drop_last=False,\n",
              " dataloader_num_workers=0,\n",
              " dataloader_persistent_workers=False,\n",
              " dataloader_pin_memory=True,\n",
              " dataloader_prefetch_factor=None,\n",
              " ddp_backend=None,\n",
              " ddp_broadcast_buffers=None,\n",
              " ddp_bucket_cap_mb=None,\n",
              " ddp_find_unused_parameters=None,\n",
              " ddp_timeout=1800,\n",
              " debug=[],\n",
              " deepspeed=None,\n",
              " disable_tqdm=False,\n",
              " dispatch_batches=None,\n",
              " do_eval=False,\n",
              " do_predict=False,\n",
              " do_train=False,\n",
              " eval_accumulation_steps=None,\n",
              " eval_delay=0,\n",
              " eval_do_concat_batches=True,\n",
              " eval_on_start=False,\n",
              " eval_steps=None,\n",
              " eval_strategy=no,\n",
              " evaluation_strategy=None,\n",
              " fp16=False,\n",
              " fp16_backend=auto,\n",
              " fp16_full_eval=False,\n",
              " fp16_opt_level=O1,\n",
              " fsdp=[],\n",
              " fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              " fsdp_min_num_params=0,\n",
              " fsdp_transformer_layer_cls_to_wrap=None,\n",
              " full_determinism=False,\n",
              " gradient_accumulation_steps=1,\n",
              " gradient_checkpointing=False,\n",
              " gradient_checkpointing_kwargs=None,\n",
              " greater_is_better=None,\n",
              " group_by_length=False,\n",
              " half_precision_backend=auto,\n",
              " hub_always_push=False,\n",
              " hub_model_id=None,\n",
              " hub_private_repo=False,\n",
              " hub_strategy=every_save,\n",
              " hub_token=<HUB_TOKEN>,\n",
              " ignore_data_skip=False,\n",
              " include_inputs_for_metrics=False,\n",
              " include_num_input_tokens_seen=False,\n",
              " include_tokens_per_second=False,\n",
              " jit_mode_eval=False,\n",
              " label_names=None,\n",
              " label_smoothing_factor=0.0,\n",
              " learning_rate=5e-05,\n",
              " length_column_name=length,\n",
              " load_best_model_at_end=False,\n",
              " local_rank=0,\n",
              " log_level=passive,\n",
              " log_level_replica=warning,\n",
              " log_on_each_node=True,\n",
              " logging_dir=KantaiBERT/runs/Jul23_00-44-14_fa4010f1990b,\n",
              " logging_first_step=False,\n",
              " logging_nan_inf_filter=True,\n",
              " logging_steps=500,\n",
              " logging_strategy=steps,\n",
              " lr_scheduler_kwargs={},\n",
              " lr_scheduler_type=linear,\n",
              " max_grad_norm=1.0,\n",
              " max_steps=-1,\n",
              " metric_for_best_model=None,\n",
              " mp_parameters=,\n",
              " neftune_noise_alpha=None,\n",
              " no_cuda=False,\n",
              " num_train_epochs=3,\n",
              " optim=adamw_torch,\n",
              " optim_args=None,\n",
              " optim_target_modules=None,\n",
              " output_dir=KantaiBERT,\n",
              " overwrite_output_dir=True,\n",
              " past_index=-1,\n",
              " per_device_eval_batch_size=8,\n",
              " per_device_train_batch_size=64,\n",
              " prediction_loss_only=False,\n",
              " push_to_hub=False,\n",
              " push_to_hub_model_id=None,\n",
              " push_to_hub_organization=None,\n",
              " push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              " ray_scope=last,\n",
              " remove_unused_columns=True,\n",
              " report_to=['tensorboard'],\n",
              " restore_callback_states_from_checkpoint=False,\n",
              " resume_from_checkpoint=None,\n",
              " run_name=KantaiBERT,\n",
              " save_on_each_node=False,\n",
              " save_only_model=False,\n",
              " save_safetensors=True,\n",
              " save_steps=10000,\n",
              " save_strategy=steps,\n",
              " save_total_limit=2,\n",
              " seed=42,\n",
              " skip_memory_metrics=True,\n",
              " split_batches=None,\n",
              " tf32=None,\n",
              " torch_compile=False,\n",
              " torch_compile_backend=None,\n",
              " torch_compile_mode=None,\n",
              " torchdynamo=None,\n",
              " tpu_metrics_debug=False,\n",
              " tpu_num_cores=None,\n",
              " use_cpu=False,\n",
              " use_ipex=False,\n",
              " use_legacy_prediction_loop=False,\n",
              " use_mps_device=False,\n",
              " warmup_ratio=0.0,\n",
              " warmup_steps=0,\n",
              " weight_decay=0.0,\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"KantaiBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "trainer, training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "xr4Xdd9iCgGb",
        "outputId": "ccdfd077-9b57-4a22-d49f-02153dca45de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8016' max='8016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8016/8016 28:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.679500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.814700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.609100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.403100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.263600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.050900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>3.965300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>3.932300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>3.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>3.798300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>3.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>3.769500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27min 26s, sys: 9.03 s, total: 27min 36s\n",
            "Wall time: 28min 27s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8016, training_loss=4.413784048990337, metrics={'train_runtime': 1706.8336, 'train_samples_per_second': 300.493, 'train_steps_per_second': 4.696, 'total_flos': 2621616775994112.0, 'train_loss': 4.413784048990337, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1YssjabjCgGb"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"KantaiBERT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4x1LNaBLCgGb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"KantaiBERT\",\n",
        "    tokenizer=\"KantaiBERT\",\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_Ih6hyZCgGb",
        "outputId": "44c6bdbf-2ec7-4995-d7db-82ae9641052d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.7162726521492004,\n",
              "  'token': 18,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'Human thinking involves human.'},\n",
              " {'score': 0.019183965399861336,\n",
              "  'token': 1031,\n",
              "  'token_str': ').',\n",
              "  'sequence': 'Human thinking involves human).'},\n",
              " {'score': 0.01650133915245533,\n",
              "  'token': 65,\n",
              "  'token_str': ']',\n",
              "  'sequence': 'Human thinking involves human]'},\n",
              " {'score': 0.012124789878726006,\n",
              "  'token': 393,\n",
              "  'token_str': ' reason',\n",
              "  'sequence': 'Human thinking involves human reason'},\n",
              " {'score': 0.00953668076545,\n",
              "  'token': 30,\n",
              "  'token_str': ':',\n",
              "  'sequence': 'Human thinking involves human:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "fill_mask(\"Human thinking involves human <mask>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-41XiEuCgGc",
        "outputId": "a5d2679c-25c9-4a28-c207-97ea38137d25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.7605432271957397,\n",
              "  'token': 18,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'The world will remember.'},\n",
              " {'score': 0.024912850931286812,\n",
              "  'token': 1031,\n",
              "  'token_str': ').',\n",
              "  'sequence': 'The world will remember).'},\n",
              " {'score': 0.01722089573740959,\n",
              "  'token': 35,\n",
              "  'token_str': '?',\n",
              "  'sequence': 'The world will remember?'},\n",
              " {'score': 0.015331648290157318,\n",
              "  'token': 30,\n",
              "  'token_str': ':',\n",
              "  'sequence': 'The world will remember:'},\n",
              " {'score': 0.00858664233237505,\n",
              "  'token': 65,\n",
              "  'token_str': ']',\n",
              "  'sequence': 'The world will remember]'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "fill_mask(\"The world will remember <mask>\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"This is the general formula for the moral <mask>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99dbRI0QMGyy",
        "outputId": "ecd58bd0-6f6c-415a-d688-9f59421b92dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2394527643918991,\n",
              "  'token': 446,\n",
              "  'token_str': ' law',\n",
              "  'sequence': 'This is the general formula for the moral law'},\n",
              " {'score': 0.040825679898262024,\n",
              "  'token': 613,\n",
              "  'token_str': ' principle',\n",
              "  'sequence': 'This is the general formula for the moral principle'},\n",
              " {'score': 0.013761923648416996,\n",
              "  'token': 610,\n",
              "  'token_str': ' practical',\n",
              "  'sequence': 'This is the general formula for the moral practical'},\n",
              " {'score': 0.01225338689982891,\n",
              "  'token': 418,\n",
              "  'token_str': ' conception',\n",
              "  'sequence': 'This is the general formula for the moral conception'},\n",
              " {'score': 0.010023773647844791,\n",
              "  'token': 600,\n",
              "  'token_str': ' understanding',\n",
              "  'sequence': 'This is the general formula for the moral understanding'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QYLSHAjaMfaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}