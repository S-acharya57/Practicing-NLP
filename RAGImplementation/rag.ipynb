{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5ab31f-759c-45de-a640-d2bfe6bb256f",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "- to retrieve relevant passages based on a query and use those passages to augment an input to an LLM so that it can generate an output based on those relevant passages\n",
    "\n",
    "1. Similarity Search/ Vector Search / Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce01b243-2e1d-4c1c-91cf-4c44f3ea9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919e266d-046f-4c5f-8ea7-9261d3995364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f004bb-239b-4bd2-b6b8-836444b06eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_character_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17</td>\n",
       "      <td>CUDA by Example g JAson sAnders edwArd KAndrot...</td>\n",
       "      <td>226</td>\n",
       "      <td>43</td>\n",
       "      <td>56.5</td>\n",
       "      <td>[-0.0277873948, -0.0197100993, -0.0666598231, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16</td>\n",
       "      <td>Many of the designations used by manufacturers...</td>\n",
       "      <td>1556</td>\n",
       "      <td>231</td>\n",
       "      <td>389.0</td>\n",
       "      <td>[0.0397266746, -0.0556121096, -0.0714852512, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-16</td>\n",
       "      <td>p. cm.  Includes index.  ISBN 978-0-13-138768-...</td>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "      <td>32.5</td>\n",
       "      <td>[0.0463345461, -0.0485776998, -0.0402859673, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -17  CUDA by Example g JAson sAnders edwArd KAndrot...   \n",
       "1          -16  Many of the designations used by manufacturers...   \n",
       "2          -16  p. cm.  Includes index.  ISBN 978-0-13-138768-...   \n",
       "\n",
       "   chunk_character_count  chunk_word_count  chunk_token_count  \\\n",
       "0                    226                43               56.5   \n",
       "1                   1556               231              389.0   \n",
       "2                    130                16               32.5   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.0277873948, -0.0197100993, -0.0666598231, ...  \n",
       "1  [0.0397266746, -0.0556121096, -0.0714852512, 0...  \n",
       "2  [0.0463345461, -0.0485776998, -0.0402859673, 0...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_chunks_embeddings_df = pd.read_csv('text_chunks_embeddings_df.csv')\n",
    "\n",
    "# convert embedding to np.array\n",
    "texts_chunks_embeddings_df[\"embedding\"] = texts_chunks_embeddings_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "pages_and_chunks = texts_chunks_embeddings_df.to_dict(orient='records')\n",
    "texts_chunks_embeddings_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20422460-ec3c-4b37-83d1-bfa5211aa0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(texts_chunks_embeddings_df.embedding.tolist(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be2e1a6b-1c9f-41f7-b4e6-2a6deeb149d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8783c5c8-c660-4b84-94b3-84d97adfbf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_12504\\552413061.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings = torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([426, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b8c0c-1ad6-43fe-89ac-08eac20a3282",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f8be5a-4d5d-4bcc-9696-3e1fc9787cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\After\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path = 'all-mpnet-base-v2',\n",
    "                                     device=device)\n",
    "embedding_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbff1c6-f1e4-4669-915e-daa8b4f800eb",
   "metadata": {},
   "source": [
    "## Semantic Searching\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Query String\n",
    "- String to Embedding\n",
    "- Dot product or Cosine Similarity between text embeddings and query embeddings\n",
    "- Sort results (top 3, descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bbaf8a5-a4c1-4daf-a6e1-0ea1a72a7664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 768]), torch.float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings.to(device)\n",
    "embeddings.shape, embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1483de45-83b6-4de1-a9cb-8e0be3cf8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter as timer\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3c89e-c147-4b16-8fbe-dddf1a9a3e78",
   "metadata": {},
   "source": [
    "## for similarity measure\n",
    "\n",
    "- If outputs from encoding are normalized embeddings, use dot product\n",
    "    - else, use cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dcd4ca4-d815-4c7b-bbd0-a506b86a6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype of embedding is: torch.float32\n",
      "Dtype of query embedding is: torch.float32\n",
      "Shape of query embedding: torch.Size([768])\n",
      "0.00013 seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.7307, 0.7260, 0.7245, 0.7038], device='cuda:0'),\n",
       "indices=tensor([ 77, 321, 286, 329], device='cuda:0'))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"CUDA memory allocation\"\n",
    "\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "\n",
    "# ensuring the dtypes match\n",
    "print(f'Dtype of embedding is: {embeddings.dtype}')\n",
    "print(f'Dtype of query embedding is: {query_embedding.dtype}')\n",
    "\n",
    "# ensuring shape of query embedding match with the overall embeddings\n",
    "print(f'Shape of query embedding: {query_embedding.shape}')\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = sentence_transformers.util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "\n",
    "end_time = timer()\n",
    "\n",
    "print(f'{end_time-start_time:.5f} seconds')\n",
    "\n",
    "\n",
    "top_n = 4\n",
    "\n",
    "top_results = torch.topk(dot_scores, k=top_n)\n",
    "print(\"\\n\\n\")\n",
    "top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93201a4a-2c92-4b98-b076-af5dd110c7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cudA c on multIPle GPus 220 The only thing remaining in the cudaHostAlloc() version of the dot product is cleanup.  HANDLE_ERROR( cudaFreeHost( a ) );   HANDLE_ERROR( cudaFreeHost( b ) );   HANDLE_ERROR( cudaFreeHost( partial_c ) );   // free events   HANDLE_ERROR( cudaEventDestroy( start ) );   HANDLE_ERROR( cudaEventDestroy( stop ) );   printf( \"Value calculated: %f\\\\n\", c );   return elapsedTime; } You will notice that no matter what flags we use with cudaHostAlloc(), the memory always gets freed in the same way. Specifically, a call to cudaFreeHost() does the trick. And that’s that!All that remains is to look at how main() ties all of this together. The first thing we need to check is whether our device supports mapping host memory. We do this the same way we checked for device overlap in the previous chapter, with a call to cudaGetDeviceProperties().int main( void ) {   cudaDeviceProp prop;   int whichDevice;   HANDLE_ERROR( cudaGetDevice( &whichDevice ) );   HANDLE_ERROR( cudaGetDeviceProperties( &prop, whichDevice ) );   if (prop.canMapHostMemory != 1) {     printf( \"Device cannot map memory.\\\\n\" );     return 0;   }'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks[329]['sentence_chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f3eb97d5-c57e-40bd-a0d3-781ef45e3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for semantic search\n",
    "def retrieve_resources(query: str,\n",
    "                       embeddings: torch.tensor, \n",
    "                       model: SentenceTransformer = embedding_model,\n",
    "                       top_k = 3,\n",
    "                       pages_and_chunks = pages_and_chunks,\n",
    "                       print_time: bool=True,\n",
    "                       display=False):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    start_time = timer()\n",
    "    dot_scores = sentence_transformers.util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f'Time taken: {(end_time - start_time):.5f} seconds')\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                k = top_k)\n",
    "\n",
    "    if display:\n",
    "        \n",
    "        for score, idx in zip(scores, indices):\n",
    "            print(f'\\nScore: {score}')\n",
    "            print(\"Text:\")\n",
    "            print(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "        \n",
    "    \n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9d72097-670d-4902-8eb7-9c2559387499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6934302449226379\n",
      "Text:\n",
      "Parallel programming (Computer science) I. Kandrot, Edward. II. Title.  QA76.76. A65S255 2010  005.2'75—dc22                                                              2010017618 Copyright © 2011 NVIDIA Corporation All rights reserved. Printed in the United States of America. This publication is protected by copy- right, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to: Pearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax: (617) 671-3447 ISBN-13: 978-0-13-138768-3 ISBN-10:    0-13-138768-5 Text printed in the United States on recycled paper at Edwards Brothers in Ann Arbor, Michigan. First printing, July 2010\n",
      "\n",
      "Score: 0.6296412944793701\n",
      "Text:\n",
      "59 Chapter 5 thread Cooperation We have now written our first program using CUDA C as well as have seen how to write code that executes in parallel on a GPU. This is an excellent start!But arguably one of the most important components to parallel programming is the means by which the parallel processing elements cooperate on solving a problem. Rare are the problems where every processor can compute results and terminate execution without a passing thought as to what the other proces- sors are doing. For even moderately sophisticated algorithms, we will need the parallel copies of our code to communicate and cooperate. So far, we have not seen any mechanisms for accomplishing this communication between sections of CUDA C code executing in parallel. Fortunately, there is a solution, one that we will begin to explore in this chapter.\n",
      "\n",
      "Score: 0.6162852644920349\n",
      "Text:\n",
      "ooperation 60 Chapter Objectives \t \t threads. You will learn a mechanism for different threads to communicate with each other.\t You will learn a mechanism to synchronize the parallel execution of different \t threads. Splitting Parallel Blocks \t In the previous chapter, we looked at how to launch parallel code on the GPU. We did this by instructing the CUDA runtime system on how many parallel copies of our kernel to launch. We call these parallel copies blocks. The CUDA runtime allows these blocks to be split into threads. Recall that when we launched multiple parallel blocks, we changed the first argument in the angle brackets from 1 to the number of blocks we wanted to launch. For example, when we studied vector addition, we launched a block for each element in the vector of size N by calling this:   add<<<N,1>>>( dev_a, dev_b, dev_c ); Inside the angle brackets, the second parameter actually represents the number of threads per block we want the CUDA runtime to create on our behalf.\n",
      "\n",
      "Score: 0.5905205607414246\n",
      "Text:\n",
      "185 Chapter 10 Streams Time and time again in this book we have seen how the massively data-parallel execution engine on a GPU can provide stunning performance gains over compa- rable CPU code. However, there is yet another class of parallelism to be exploited on NVIDIA graphics processors. This parallelism is similar to the task parallelism that is found in multithreaded CPU applications. Rather than simultaneously computing the same function on lots of data elements as one does with data parallelism, task parallelism involves doing two or more completely different tasks in parallel. In the context of parallelism, a task could be any number of things. For example, an application could be executing two tasks: redrawing its GUI with one thread while downloading an update over the network with another thread. These tasks proceed in parallel, despite having nothing in common. Although the task paral- lelism on GPUs is not currently as flexible as a general-purpose processor’s, it still provides opportunities for us as programmers to extract even more speed from our GPU-based implementations. In this chapter, we will look at CUDA streams and the ways in which their careful use will enable us to execute certain operations simultaneously on the GPU.\n",
      "\n",
      "Score: 0.5863679647445679\n",
      "Text:\n",
      "1 Chapter 1 Why CUDA?Why Now?There was a time in the not-so-distant past when parallel computing was looked upon as an “exotic” pursuit and typically got compartmentalized as a specialty within the field of computer science. This perception has changed in profound ways in recent years. The computing world has shifted to the point where, far from being an esoteric pursuit, nearly every aspiring programmer needs training in parallel programming to be fully effective in computer science. Perhaps you’ve picked this book up unconvinced about the importance of parallel programming in the computing world today and the increasingly large role it will play in the years to come. This introductory chapter will examine recent trends in the hard- ware that does the heavy lifting for the software that we as programmers write. In doing so, we hope to convince you that the parallel computing revolution has already happened and that, by learning CUDA C, you’ll be well positioned to write high-performance applications for heterogeneous platforms that contain both central and graphics processing units.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6934, 0.6296, 0.6163, 0.5905, 0.5864], device='cuda:0'),\n",
       " tensor([  3, 107, 108, 285,  26], device='cuda:0'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_resources(query=\"parallel processing\", embeddings=embeddings, pages_and_chunks=pages_and_chunks, print_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "603ef7dc-cfa9-41cd-b54e-7248665e1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 6 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory/(2**30))\n",
    "print(f'GPU Memory: {memory_gb} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ad2ed-d488-42c6-a28e-e62ea67bda65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a513c51-f49f-4a6d-a997-8ec06422054e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef718a-cdba-4ea3-ade9-9dc2d65387d2",
   "metadata": {},
   "source": [
    "### Using Google Gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fda85e6e-f177-4445-a40c-f7778bdfafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "519857fc-7322-49c8-b264-b37a31ab975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_flash_attn_2_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed85d240-d5b0-433f-9ebf-79fec5b09d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdpa'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (is_flash_attn_2_available()):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\" \n",
    "# sdpa = scaled dot product attention\n",
    "attn_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60b700e6-f895-4a39-a542-f9370a5b6738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2-2b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t5: AddedToken(\"<2mass>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t6: AddedToken(\"[@BOS@]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t7: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t8: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t9: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t10: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t11: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t12: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t13: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t14: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t15: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t16: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t17: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t18: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t19: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t20: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t21: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t22: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t23: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t24: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t25: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t26: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t27: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t28: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t29: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t30: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t31: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t33: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t34: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t35: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t36: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t37: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t38: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t39: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t40: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t41: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t42: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t43: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t44: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t45: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t46: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t47: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t48: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t49: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t50: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t51: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t52: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t53: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t54: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t55: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t56: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t57: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t58: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t59: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t60: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t61: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t62: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t63: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t64: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t65: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t66: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t67: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t68: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t69: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t70: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t71: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t72: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t73: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t74: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t75: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t76: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t77: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t78: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t79: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t80: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t81: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t82: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t83: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t84: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t85: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t86: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t87: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t88: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t89: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t90: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t91: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t92: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t93: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t94: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t95: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t96: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t97: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t98: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t99: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t100: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t101: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t103: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t104: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t105: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t109: AddedToken(\"\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t110: AddedToken(\"\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t111: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t112: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t113: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t114: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t115: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t116: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t117: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t118: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t119: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t120: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t121: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t122: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t123: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t124: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t125: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t126: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t127: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t129: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t130: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t131: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t132: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t133: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t134: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t135: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t136: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t137: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t138: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t139: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t140: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t141: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t142: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t143: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t144: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t145: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t146: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t147: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t148: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t149: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t150: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t152: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t153: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t154: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t155: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t156: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t157: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t158: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t159: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t160: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t161: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t162: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t163: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t164: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t165: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t166: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t167: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t168: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t169: AddedToken(\"<table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t170: AddedToken(\"<caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t171: AddedToken(\"<thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t172: AddedToken(\"<tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t173: AddedToken(\"<tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t174: AddedToken(\"<tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t175: AddedToken(\"<th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t176: AddedToken(\"<td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t177: AddedToken(\"</table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t178: AddedToken(\"</caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t179: AddedToken(\"</thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t180: AddedToken(\"</tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t181: AddedToken(\"</tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t182: AddedToken(\"</tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t183: AddedToken(\"</th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t184: AddedToken(\"</td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t185: AddedToken(\"<h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t186: AddedToken(\"<h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t187: AddedToken(\"<h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t188: AddedToken(\"<h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t189: AddedToken(\"<h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t190: AddedToken(\"<h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t191: AddedToken(\"<blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t192: AddedToken(\"</h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t193: AddedToken(\"</h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t194: AddedToken(\"</h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t195: AddedToken(\"</h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t196: AddedToken(\"</h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t197: AddedToken(\"</h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t198: AddedToken(\"</blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t199: AddedToken(\"<strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t200: AddedToken(\"<em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t201: AddedToken(\"<b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t202: AddedToken(\"<i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t203: AddedToken(\"<u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t204: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t205: AddedToken(\"<sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t206: AddedToken(\"<sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t207: AddedToken(\"<code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t208: AddedToken(\"</strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t209: AddedToken(\"</em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t210: AddedToken(\"</b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t211: AddedToken(\"</i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t212: AddedToken(\"</u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t213: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t214: AddedToken(\"</sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t215: AddedToken(\"</sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t216: AddedToken(\"</code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255968: AddedToken(\"[toxicity=0]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255969: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255970: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255971: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255972: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255973: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255974: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255975: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255976: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255977: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255978: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255979: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255980: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255981: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255982: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255983: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255984: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255985: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255986: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255987: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255988: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255989: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255990: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255991: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255992: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255993: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255994: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255995: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255996: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255997: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255998: AddedToken(\"\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t255999: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "263d4b6c-d527-49bc-8f18-bd27624c12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███| 2/2 [00:11<00:00,  5.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = \"google/gemma-2-2b-it\", \n",
    "                                            torch_dtype = torch.float16, \n",
    "                                            quantization_config = None,               \n",
    "                                            attn_implementation = attn_implementation)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ad16f06a-794d-479f-8830-8564257fa020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9cd50ac9-dd39-4bc2-99b2-da27b30c5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 4.869603633880615 GB\n"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "\n",
    "model_mem_gb = model_mem_bytes / (1024**3)\n",
    "\n",
    "print(f'Model memory: {model_mem_gb} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "631551ae-78cc-4adf-a653-2bc7df77b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \n",
      "<bos><start_of_turn>user\n",
      "What is CUDA parallel processing? how is GPU constructed?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"What is CUDA parallel processing? how is GPU constructed?\"\n",
    "\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": text}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt: \\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "402bdada-5513-4b0c-8dc0-572c00fcb275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   1841,    603, 154144,  14255,\n",
      "          10310, 235336,   1368,    603,  37783,  18871, 235336,    107,    108,\n",
      "            106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenizing input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a1e82dc5-2868-4edf-a615-7695ee46ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\After\\torch\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:516: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  is_causal = True if causal_mask is None and q_len > 1 else False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 52s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2f15c28-de17-42fc-ba32-0517a51e8581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "What is CUDA parallel processing? how is GPU constructed?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Let's break down CUDA parallel processing and how GPUs are built.\n",
      "\n",
      "**What is CUDA Parallel Processing?**\n",
      "\n",
      "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It allows software developers to harness the power of NVIDIA GPUs (Graphics Processing Units) for general-purpose computing tasks, not just graphics rendering. \n",
      "\n",
      "Here's the core idea:\n",
      "\n",
      "* **GPUs are designed for parallel processing:**  GPUs have thousands of cores, each capable of performing simple calculations simultaneously. This is in contrast to CPUs, which are designed for sequential processing.\n",
      "* **CUDA enables software to utilize these cores:**  CUDA allows developers to write code that can be executed on the GPU, taking advantage of its parallel architecture.\n",
      "* **Benefits of CUDA:**\n",
      "    * **Speed:**  GPUs are incredibly fast at performing massive parallel calculations, making them ideal for tasks like scientific simulations, machine learning, and data analysis.\n",
      "    * **Efficiency:**  GPUs can often outperform CPUs for specific types of computations, especially when dealing with large datasets.\n",
      "    * **Scalability:**  GPUs can be easily scaled up to handle even more complex and demanding tasks.\n",
      "\n",
      "**How GPUs are Constructed**\n",
      "\n",
      "GPUs are\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 68.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4da65a-2f16-49aa-9110-0b17916075e4",
   "metadata": {},
   "source": [
    "### Managing Prompt to pass into LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7fdf3cdf-b3c0-4566-ba8b-20e6f9458c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(query: str,\n",
    "                 context_items: list[dict]) -> str :\n",
    "    context = f\"- \" + f\"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    base_prompt = \"\"\"Based on the context items, please answer the query:\n",
    "    Context_items: {context}\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    prompt = base_prompt.format(context=context, \n",
    "                               query=query)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "13e93dbc-6cf1-4438-801c-b850e1bfd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00004 seconds\n",
      "\n",
      "Score: 0.6080108880996704\n",
      "Text:\n",
      "Parallel programming (Computer science) I. Kandrot, Edward. II. Title.  QA76.76. A65S255 2010  005.2'75—dc22                                                              2010017618 Copyright © 2011 NVIDIA Corporation All rights reserved. Printed in the United States of America. This publication is protected by copy- right, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to: Pearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax: (617) 671-3447 ISBN-13: 978-0-13-138768-3 ISBN-10:    0-13-138768-5 Text printed in the United States on recycled paper at Edwards Brothers in Ann Arbor, Michigan. First printing, July 2010\n",
      "\n",
      "Score: 0.5896118879318237\n",
      "Text:\n",
      "ooperation 60 Chapter Objectives \t \t threads. You will learn a mechanism for different threads to communicate with each other.\t You will learn a mechanism to synchronize the parallel execution of different \t threads. Splitting Parallel Blocks \t In the previous chapter, we looked at how to launch parallel code on the GPU. We did this by instructing the CUDA runtime system on how many parallel copies of our kernel to launch. We call these parallel copies blocks. The CUDA runtime allows these blocks to be split into threads. Recall that when we launched multiple parallel blocks, we changed the first argument in the angle brackets from 1 to the number of blocks we wanted to launch. For example, when we studied vector addition, we launched a block for each element in the vector of size N by calling this:   add<<<N,1>>>( dev_a, dev_b, dev_c ); Inside the angle brackets, the second parameter actually represents the number of threads per block we want the CUDA runtime to create on our behalf.\n",
      "\n",
      "Score: 0.5503892302513123\n",
      "Text:\n",
      "185 Chapter 10 Streams Time and time again in this book we have seen how the massively data-parallel execution engine on a GPU can provide stunning performance gains over compa- rable CPU code. However, there is yet another class of parallelism to be exploited on NVIDIA graphics processors. This parallelism is similar to the task parallelism that is found in multithreaded CPU applications. Rather than simultaneously computing the same function on lots of data elements as one does with data parallelism, task parallelism involves doing two or more completely different tasks in parallel. In the context of parallelism, a task could be any number of things. For example, an application could be executing two tasks: redrawing its GUI with one thread while downloading an update over the network with another thread. These tasks proceed in parallel, despite having nothing in common. Although the task paral- lelism on GPUs is not currently as flexible as a general-purpose processor’s, it still provides opportunities for us as programmers to extract even more speed from our GPU-based implementations. In this chapter, we will look at CUDA streams and the ways in which their careful use will enable us to execute certain operations simultaneously on the GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"- Parallel programming (Computer science) I. Kandrot, Edward. II. Title.  QA76.76. A65S255 2010  005.2'75—dc22                                                              2010017618 Copyright © 2011 NVIDIA Corporation All rights reserved. Printed in the United States of America. This publication is protected by copy- right, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to: Pearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax: (617) 671-3447 ISBN-13: 978-0-13-138768-3 ISBN-10:    0-13-138768-5 Text printed in the United States on recycled paper at Edwards Brothers in Ann Arbor, Michigan. First printing, July 2010\\n- ooperation 60 Chapter Objectives \\t \\t threads. You will learn a mechanism for different threads to communicate with each other.\\t You will learn a mechanism to synchronize the parallel execution of different \\t threads. Splitting Parallel Blocks \\t In the previous chapter, we looked at how to launch parallel code on the GPU. We did this by instructing the CUDA runtime system on how many parallel copies of our kernel to launch. We call these parallel copies blocks. The CUDA runtime allows these blocks to be split into threads. Recall that when we launched multiple parallel blocks, we changed the first argument in the angle brackets from 1 to the number of blocks we wanted to launch. For example, when we studied vector addition, we launched a block for each element in the vector of size N by calling this:   add<<<N,1>>>( dev_a, dev_b, dev_c ); Inside the angle brackets, the second parameter actually represents the number of threads per block we want the CUDA runtime to create on our behalf.\\n- 185 Chapter 10 Streams Time and time again in this book we have seen how the massively data-parallel execution engine on a GPU can provide stunning performance gains over compa- rable CPU code. However, there is yet another class of parallelism to be exploited on NVIDIA graphics processors. This parallelism is similar to the task parallelism that is found in multithreaded CPU applications. Rather than simultaneously computing the same function on lots of data elements as one does with data parallelism, task parallelism involves doing two or more completely different tasks in parallel. In the context of parallelism, a task could be any number of things. For example, an application could be executing two tasks: redrawing its GUI with one thread while downloading an update over the network with another thread. These tasks proceed in parallel, despite having nothing in common. Although the task paral- lelism on GPUs is not currently as flexible as a general-purpose processor’s, it still provides opportunities for us as programmers to extract even more speed from our GPU-based implementations. In this chapter, we will look at CUDA streams and the ways in which their careful use will enable us to execute certain operations simultaneously on the GPU.\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"What is parallel processing?\",\n",
    "          \"Tell me about heat transfer\",\n",
    "          \"What is the best way to use GPU for tensors multiplication with programming?\"]\n",
    "\n",
    "query = random.choice(queries)\n",
    "\n",
    "scores, indices = retrieve_resources(query=query,\n",
    "                                    embeddings=embeddings)\n",
    "\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "prompt = format_prompt(query=query, context_items=context_items)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dcd5789e-939a-41f7-9498-2d9ad3d23b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\tWhat is parallel processing?\n",
      "RAG Answer: <bos>\n",
      "- Parallel programming (Computer science) I. Kandrot, Edward. II. Title.  QA76.76. A65S255 2010  005.2'75—dc22                                                              2010017618 Copyright © 2011 NVIDIA Corporation All rights reserved. Printed in the United States of America. This publication is protected by copy- right, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to: Pearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax: (617) 671-3447 ISBN-13: 978-0-13-138768-3 ISBN-10:    0-13-138768-5 Text printed in the United States on recycled paper at Edwards Brothers in Ann Arbor, Michigan. First printing, July 2010\n",
      " ooperation 60 Chapter Objectives \t \t threads. You will learn a mechanism for different threads to communicate with each other.\t You will learn a mechanism to synchronize the parallel execution of different \t threads. Splitting Parallel Blocks \t In the previous chapter, we looked at how to launch parallel code on the GPU. We did this by instructing the CUDA runtime system on how many parallel copies of our kernel to launch. We call these parallel copies blocks. The CUDA runtime allows these blocks to be split into threads. Recall that when we launched multiple parallel blocks, we changed the first argument in the angle brackets from 1 to the number of blocks we wanted to launch. For example, when we studied vector addition, we launched a block for each element in the vector of size N by calling this:   add<<<N,1>>>( dev_a, dev_b, dev_c ); Inside the angle brackets, the second parameter actually represents the number of threads per block we want the CUDA runtime to create on our behalf.\n",
      " 185 Chapter 10 Streams Time and time again in this book we have seen how the massively data-parallel execution engine on a GPU can provide stunning performance gains over compa- rable CPU code. However, there is yet another class of parallelism to be exploited on NVIDIA graphics processors. This parallelism is similar to the task parallelism that is found in multithreaded CPU applications. Rather than simultaneously computing the same function on lots of data elements as one does with data parallelism, task parallelism involves doing two or more completely different tasks in parallel. In the context of parallelism, a task could be any number of things. For example, an application could be executing two tasks: redrawing its GUI with one thread while downloading an update over the network with another thread. These tasks proceed in parallel, despite having nothing in common. Although the task paral- lelism on GPUs is not currently as flexible as a general-purpose processor’s, it still provides opportunities for us as programmers to extract even more speed from our GPU-based implementations. In this chapter, we will look at CUDA streams and the ways in which their careful use will enable us to execute certain operations simultaneously on the GPU.\n",
      " In the search for additional processing power for personal computers, the improvement in supercomputers raises a very good question: Rather than solely looking to increase the performance of a single processing core, why not put more than one in a personal computer?In this way, personal computers could continue to improve in performance without the need for continuing increases in processor clock speed. In 2005, faced with an increasingly competitive marketplace and few alternatives, leading CPU manufacturers began offering processors with two computing cores instead of one. Over the following years, they followed this development with the release of three-, four-, six-, and eight-core central processor units. Sometimes referred to as the multicore revolution, this trend has marked a huge shift in the evolution of the consumer computing market. Today, it is relatively challenging to purchase a desktop computer with a CPU containing but a single computing core. Even low-end, low-power central proces- sors ship with two or more cores per die. Leading CPU manufacturers have already announced plans for 12- and 16-core CPUs, further confirming that parallel computing has arrived for good.\n",
      " 59 Chapter 5 thread Cooperation We have now written our first program using CUDA C as well as have seen how to write code that executes in parallel on a GPU. This is an excellent start!But arguably one of the most important components to parallel programming is the means by which the parallel processing elements cooperate on solving a problem. Rare are the problems where every processor can compute results and terminate execution without a passing thought as to what the other proces- sors are doing. For even moderately sophisticated algorithms, we will need the parallel copies of our code to communicate and cooperate. So far, we have not seen any mechanisms for accomplishing this communication between sections of CUDA C code executing in parallel. Fortunately, there is a solution, one that we will begin to explore in this chapter.\n",
      " 18 Chapter 10 Streams Time and time again in this book we have seen how the massively data-parallel execution engine on a GPU can provide stunning performance gains over compa- rable CPU code. However, there is yet another class of parallelism to be exploited on NVIDIA graphics processors. This parallelism is similar to the task parallelism that is found in multithreaded CPU applications. Rather than simultaneously computing the same function on lots of data elements as one does with data parallelism, task parallelism involves doing two or more completely different tasks in parallel. In the context of parallelism, a task could be any number of things. For example, an application could be executing two tasks: redrawing its GUI with one thread while downloading an update over the network with another thread. These tasks proceed in parallel, despite having nothing in common. Although the task paral- lelism on GPUs is not currently as flexible as a general-purpose processor’s, it still provides opportunities for us as programmers to extract even more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "               max_new_tokens=200)\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f'Query:\\t{query}')\n",
    "print(f'RAG Answer: {output_text.replace(prompt, '')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cca78ea2-715d-4d62-8b0b-b7169727bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\tWhat is parallel processing?\n",
      "RAG Answer: <bos> \n",
      "\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "This document appears to be a chapter excerpt from a book titled \"Parallel Programming (Computer Science)\" by Kandrot and Edward. \n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "* **Parallel Programming:** The chapter focuses on parallel programming techniques, particularly on how to utilize the GPU for parallel execution.\n",
      "* **CUDA Runtime:** The CUDA runtime system is discussed, which manages the launch of parallel blocks and threads.\n",
      "* **Threads and Blocks:** The relationship between threads and blocks is explained, with the number of threads per block being controlled by the second argument in the CUDA launch function.\n",
      "* **Task Parallelism:** The chapter introduces task parallelism, a different type of parallelism where multiple tasks are executed concurrently, even if they are unrelated.\n",
      "* **CUDA Streams:** The chapter concludes by introducing CUDA streams, which allow for the execution of operations in parallel.\n",
      "\n",
      "\n",
      "**Overall:** This excerpt provides a foundational understanding of parallel programming concepts, specifically focusing on the\n",
      "CPU times: total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with passing query with the context\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "               max_new_tokens=200)\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f'Query:\\t{query}')\n",
    "print(f\"RAG Answer: {output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954fe50-02cb-4ece-895e-93f8f5c06034",
   "metadata": {},
   "source": [
    "### LLM Model with Context Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6176f050-2c0f-4856-87b2-696a6e3c4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str,\n",
    "       max_new_tokens: int = 256):\n",
    "    scores, indices = retrieve_resources(query=query,\n",
    "                                         embeddings=embeddings)\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu()\n",
    "\n",
    "    prompt = format_prompt(query=query,\n",
    "                          context_items=context_items)\n",
    "    # tokenizing\n",
    "    input_ids = tokenizer(prompt, \n",
    "                         return_tensors = \"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(**input_ids,\n",
    "                            max_new_tokens=max_new_tokens)\n",
    "\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    output_text = output_text.replace(prompt, '').replace(\"<bos>\", '').replace(\"<end_of_turn>\", '').replace(\"\\n\", \"\")\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e6657e9e-7c1e-456a-ab97-832eb5696c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What is parallel processing?\",\n",
    "          \"What is CUDA?\",\n",
    "          \"Explain about cuda programming.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bc8046e7-9459-4c89-8daa-b6200e136fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:What is parallel processing?\n",
      "Time taken: 0.00005 seconds\n",
      "CPU times: total: 2min 14s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\n\\n**Summary:**\\n\\nThis document appears to be a chapter excerpt from a book titled \"Parallel Programming (Computer Science)\" by Kandrot and Edward. \\n\\nHere\\'s a breakdown of the key points:\\n\\n* **Parallel Programming:** The chapter focuses on parallel programming techniques, particularly on how to utilize the GPU for parallel execution.\\n* **CUDA Runtime:** The CUDA runtime system is discussed, which manages the launch of parallel blocks and threads.\\n* **Threads and Blocks:** The relationship between threads and blocks is explained, with the number of threads per block being controlled by the second argument in the CUDA launch function.\\n* **Task Parallelism:** The chapter introduces task parallelism, a different type of parallelism where multiple tasks are executed concurrently, even if they are unrelated.\\n* **CUDA Streams:** The chapter concludes by introducing CUDA streams, which allow for the execution of operations in parallel.\\n\\n\\n**Overall:** This excerpt provides a foundational understanding of parallel programming concepts, specifically focusing on the use of CUDA streams for concurrent execution on NVIDIA GPUs. \\n<end_of_turn>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = random.choice(queries)\n",
    "print(f'Query:{query}')\n",
    "ask(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1ca3273f-aeaa-4a79-96a6-2878605f776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:Explain about cuda programming.\n",
      "Time taken: 0.00004 seconds\n",
      "CPU times: total: 2min 42s\n",
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Summary:**This chapter is an introduction to CUDA C programming. It emphasizes the importance of understanding the CUDA Architecture and the nuances of NVIDIA GPUs. It recommends a book for further learning and suggests that a basic understanding of C or C++ is helpful. The chapter also mentions setting up the development environment for CUDA C and then moving on to the next chapter.**Key Points:*** **CUDA C Programming:** The chapter introduces CUDA C programming as a way to develop code for NVIDIA GPUs.* **CUDA Architecture:** Understanding the CUDA Architecture is crucial for advanced CUDA C programming.* **NVIDIA GPUs:** The chapter highlights the importance of understanding how NVIDIA GPUs work.* **Programming Massively Parallel Processors:** A recommended book for deeper understanding of CUDA Architecture.* **Prerequisites:** Basic knowledge of C or C++ is helpful for understanding the concepts.* **Development Environment:** Setting up the development environment is necessary for CUDA C programming.* **Next Chapter:** The chapter encourages readers to skip to Chapter 3 if they have prior experience with CUDA C or a properly set up development environment.**Overall:**This chapter provides a good overview of CUDA C programming and sets the stage for further learning. It emphasizes the importance of understanding the underlying'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = random.choice(queries)\n",
    "print(f'Query:{query}')\n",
    "ask(query=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
