{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e97aaf-a2b0-46c9-9881-75f1c45e488f",
   "metadata": {},
   "source": [
    "## Document Preprocessing and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c722eee-407a-48b0-86d1-c76d11cbf9de",
   "metadata": {},
   "source": [
    "#### Using PDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f043350-9597-4377-9168-97c5d012c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "pdf_path = \"cuda_book.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f'[INFO] File{pdf_path} doesn\\'t exist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0e9d000-d489-472a-8cb2-a78617e7cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\After\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz \n",
    "from tqdm.auto import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db371343-4c98-4a8c-a90c-4bb2ef85188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text:str)->str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fed7464e-f03e-411d-b82e-1760c4c83153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_read_pdf(pdf_path:str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_texts = []\n",
    "\n",
    "    for page_number, content in tqdm(enumerate(doc)):\n",
    "        text = content.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_texts.append({\"page no\":page_number-20,\n",
    "                           \"page_char_count\": len(text),\n",
    "                           \"page_word_count\" : len(text.split(\" \")), \n",
    "                           \"page_sentence_count_raw\" : len(text.split(\". \")), \n",
    "                           \"page_token_count\" : len(text)/4 , # 1 token = 4 characters\n",
    "                           \"text\" : text })\n",
    "\n",
    "    return pages_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79b90ae-81a9-43c8-be94-74139c14a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [00:00, 487.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page no': -20,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page no': -19,\n",
       "  'page_char_count': 15,\n",
       "  'page_word_count': 3,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 3.75,\n",
       "  'text': 'CUDA by Example'},\n",
       " {'page no': -18,\n",
       "  'page_char_count': 34,\n",
       "  'page_word_count': 5,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 8.5,\n",
       "  'text': 'This page intentionally left blank'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts = open_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f5dc8c-4702-48bd-9c5a-f8a2b75c3107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page no': 189,\n",
       "  'page_char_count': 1314,\n",
       "  'page_word_count': 431,\n",
       "  'page_sentence_count_raw': 4,\n",
       "  'page_token_count': 328.5,\n",
       "  'text': 'EEE LLL KKK DDD HHH SSSTTTM MM emory 189 emory After the 100 copies, clean up by freeing the host and GPU buffers as well as  destroying our timing events.     free( a );     HANDLE_ERROR( cudaFree( dev_a ) );      HANDLE_ERROR( cudaEventDestroy( start ) );     HANDLE_ERROR( cudaEventDestroy( stop ) );     return elapsedTime;  } If you didn’t notice, the function cuda_malloc_test() allocated pageable host  memory with the standard C malloc() routine. The pinned memory version  uses cudaHostAlloc() to allocate a page-locked buffer. float cuda_host_alloc_test( int size, bool up ) {     cudaEvent_t     start, stop;     int             *a, *dev_a;     float           elapsedTime;     HANDLE_ERROR( cudaEventCreate( &start ) );     HANDLE_ERROR( cudaEventCreate( &stop ) );     HANDLE_ERROR( cudaHostAlloc( (void**)&a,                                  size * sizeof( *a ),                                  cudaHostAllocDefault ) );     HANDLE_ERROR( cudaMalloc( (void**)&dev_a,                               size * sizeof( *dev_a ) ) );     HANDLE_ERROR( cudaEventRecord( start, 0 ) );     for (int i=0; i<100; i++) {         if (up)             HANDLE_ERROR( cudaMemcpy( dev_a, a,                                   size * sizeof( *a ),                                   cudaMemcpyHostToDevice ) );        else'},\n",
       " {'page no': 267,\n",
       "  'page_char_count': 1883,\n",
       "  'page_word_count': 349,\n",
       "  'page_sentence_count_raw': 15,\n",
       "  'page_token_count': 470.75,\n",
       "  'text': '267 able You probably noticed that we are using NULL as the value for every key/value pair.  In a typical application, you would likely store some useful data with the key, but  because we are primarily concerned with the hash table implementation itself,  we’re storing a meaningless value with each key.  A.2.3 MULTITHREADED HASH TABLE There are some assumptions built into our CPU hash table that will no longer be  valid when we move to the GPU. First, we have assumed that only one node can  be added to the table at a time in order to make the addition of a node simpler. If  more than one thread were trying to add a node to the table at once, we could end  up with problems similar to the multithreaded addition problems in the example  from Chapter 9. For example, let’s revisit our “avocado and aardvark” example and imagine that  threads A and B are trying to add these entries to the table. Thread A computes a  hash function on avocado, and thread B computes the function on aardvark. They  both decide their keys belong in the same bucket. To add the new entry to the list,  thread A and B start by setting their new entry’s next pointer to the first node of  the existing list as in Figure A.4. Then, both threads try to replace the entry in the bucket array with their new  entry. However, the thread that finishes second is the only thread that has its  update preserved because it overwrites the work of the previous thread. So  consider the scenario where thread A replaces the entry altitude with its entry for  avocado. Immediately after finishing, thread B replaces what it believe to be the  entry for altitude with its entry for aardvark. Unfortunately, it’s replacing avocado instead of altitude, resulting in the situation illustrated in Figure A.5. aardvark altitude audience avocado Figure A.4  Multiple threads attempting to add a node to the same bucket'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "random.sample(pages_and_texts, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "706bf425-c690-44fc-8fbf-75b8824e23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6fcaad0-8ab0-47e9-b5d4-80fe3791c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page no</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>199</td>\n",
       "      <td>1450</td>\n",
       "      <td>268</td>\n",
       "      <td>8</td>\n",
       "      <td>362.50</td>\n",
       "      <td>usInG multIPle cudA streAms 199 tream s broke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>203</td>\n",
       "      <td>1641</td>\n",
       "      <td>797</td>\n",
       "      <td>2</td>\n",
       "      <td>410.25</td>\n",
       "      <td>usInG multIPle cudA streAms 203 tream s     //...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>281</td>\n",
       "      <td>3052</td>\n",
       "      <td>439</td>\n",
       "      <td>5</td>\n",
       "      <td>763.00</td>\n",
       "      <td>ndex 281 copy_constant_kernel(), computing  te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>279</td>\n",
       "      <td>2134</td>\n",
       "      <td>316</td>\n",
       "      <td>2</td>\n",
       "      <td>533.50</td>\n",
       "      <td>279 Index A add() function, CPU vector sums, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     page no  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "219      199             1450              268                        8   \n",
       "223      203             1641              797                        2   \n",
       "301      281             3052              439                        5   \n",
       "299      279             2134              316                        2   \n",
       "\n",
       "     page_token_count                                               text  \n",
       "219            362.50  usInG multIPle cudA streAms 199 tream s broke ...  \n",
       "223            410.25  usInG multIPle cudA streAms 203 tream s     //...  \n",
       "301            763.00  ndex 281 copy_constant_kernel(), computing  te...  \n",
       "299            533.50  279 Index A add() function, CPU vector sums, 4...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "172f7efa-5d0e-4c04-b4ce-8b962f29c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page no</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>311.00</td>\n",
       "      <td>311.00</td>\n",
       "      <td>311.00</td>\n",
       "      <td>311.00</td>\n",
       "      <td>311.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>135.00</td>\n",
       "      <td>1549.69</td>\n",
       "      <td>332.66</td>\n",
       "      <td>12.72</td>\n",
       "      <td>387.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.92</td>\n",
       "      <td>666.50</td>\n",
       "      <td>135.34</td>\n",
       "      <td>35.82</td>\n",
       "      <td>166.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.50</td>\n",
       "      <td>1189.00</td>\n",
       "      <td>255.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>297.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>135.00</td>\n",
       "      <td>1531.00</td>\n",
       "      <td>357.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>382.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>212.50</td>\n",
       "      <td>1977.00</td>\n",
       "      <td>415.50</td>\n",
       "      <td>13.00</td>\n",
       "      <td>494.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>290.00</td>\n",
       "      <td>3329.00</td>\n",
       "      <td>798.00</td>\n",
       "      <td>499.00</td>\n",
       "      <td>832.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page no  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count   311.00           311.00           311.00                   311.00   \n",
       "mean    135.00          1549.69           332.66                    12.72   \n",
       "std      89.92           666.50           135.34                    35.82   \n",
       "min     -20.00             0.00             1.00                     1.00   \n",
       "25%      57.50          1189.00           255.50                     5.00   \n",
       "50%     135.00          1531.00           357.00                     8.00   \n",
       "75%     212.50          1977.00           415.50                    13.00   \n",
       "max     290.00          3329.00           798.00                   499.00   \n",
       "\n",
       "       page_token_count  \n",
       "count            311.00  \n",
       "mean             387.42  \n",
       "std              166.63  \n",
       "min                0.00  \n",
       "25%              297.25  \n",
       "50%              382.75  \n",
       "75%              494.25  \n",
       "max              832.25  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ed761-9def-4ad3-8396-20e7f43ff87b",
   "metadata": {},
   "source": [
    "## Why token counts?\n",
    "\n",
    "- Embedding models don't deal with infinite tokens\n",
    "- LLMs need finite tokens in their context window  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8067681-995d-4d7e-8ca3-1187654f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences count \n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40682f9d-b5d2-43cc-afba-20160b0a7409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1ad592a5810>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = English()\n",
    "nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d96ca865-827c-4b12-9a70-118d0c24db9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1ad598911d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\") # to split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a94af7ac-d713-492f-86a9-bd09e53bc9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A dog eats bone., A cat eats rat., A rat eats grains.]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nlp(\"A dog eats bone. A cat eats rat. A rat eats grains.\")\n",
    "list(sentences.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d8b0a3b-725d-4ac7-a0f2-0b639dafe138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 311/311 [00:00<00:00, 338.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(pages_and_texts):\n",
    "    text[\"sentences\"] = list(nlp(text[\"text\"]).sents)\n",
    "\n",
    "    text[\"sentences\"] = [str(sentence) for sentence in text[\"sentences\"]]\n",
    "\n",
    "    text[\"sentences_count_spacy\"] = len(text[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e543c239-5de6-41c5-9cd1-0ff949e9fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page no': 271,\n",
       "  'page_char_count': 1747,\n",
       "  'page_word_count': 501,\n",
       "  'page_sentence_count_raw': 9,\n",
       "  'page_token_count': 436.75,\n",
       "  'text': '271 able     if (count != ELEMENTS)         printf( “%d elements found in hash table.  Should be %ld\\\\n”,                 count, ELEMENTS );     else         printf( “All %d elements found in hash table.\\\\n”, count );     free( table.pool );     free( table.entries );  } Since we chose to reuse our CPU implementation of verify_table(), we need a  function to copy the table from GPU memory to host memory. There are three steps  to this function, two relatively obvious steps and a third, trickier step. The first two  steps involve allocating host memory for the hash table data and performing a copy  of the GPU data structures into this memory with cudaMemcpy(). We have done  this many times previously, so this should come as no surprise. void copy_table_to_host( const Table &table, Table &hostTable) {     hostTable.count = table.count;     hostTable.entries = (Entry**)calloc( table.count,                                          sizeof(Entry*) );     hostTable.pool = (Entry*)malloc( ELEMENTS *                                      sizeof( Entry ) );     HANDLE_ERROR( cudaMemcpy( hostTable.entries, table.entries,                               table.count * sizeof(Entry*),                               cudaMemcpyDeviceToHost ) );     HANDLE_ERROR( cudaMemcpy( hostTable.pool, table.pool,                               ELEMENTS * sizeof( Entry ),                               cudaMemcpyDeviceToHost ) ); The tricky portion of this routine involves the fact that some of the data we have  copied are pointers. We cannot simply copy these pointers to the host because  they are addresses on the GPU; they will no longer be valid pointers on the host.  However, the relative offsets of the pointers will still be valid. Every GPU pointer',\n",
       "  'sentences': ['271 able     if (count !',\n",
       "   '= ELEMENTS)         printf( “%d elements found in hash table.',\n",
       "   ' Should be %ld\\\\n”,                 count, ELEMENTS );     else         printf( “All %d elements found in hash table.\\\\n”, count );     free( table.pool );     free( table.entries );  } Since we chose to reuse our CPU implementation of verify_table(), we need a  function to copy the table from GPU memory to host memory.',\n",
       "   'There are three steps  to this function, two relatively obvious steps and a third, trickier step.',\n",
       "   'The first two  steps involve allocating host memory for the hash table data and performing a copy  of the GPU data structures into this memory with cudaMemcpy().',\n",
       "   'We have done  this many times previously, so this should come as no surprise.',\n",
       "   'void copy_table_to_host( const Table &table, Table &hostTable) {     hostTable.count = table.count;     hostTable.entries = (Entry**)calloc( table.count,                                          sizeof(Entry*) );     hostTable.pool = (Entry*)malloc( ELEMENTS *                                      sizeof( Entry ) );     HANDLE_ERROR( cudaMemcpy( hostTable.entries, table.entries,                               table.count * sizeof(Entry*),                               cudaMemcpyDeviceToHost ) );     HANDLE_ERROR( cudaMemcpy( hostTable.pool, table.pool,                               ELEMENTS * sizeof( Entry ),                               cudaMemcpyDeviceToHost ) ); The tricky portion of this routine involves the fact that some of the data we have  copied are pointers.',\n",
       "   'We cannot simply copy these pointers to the host because  they are addresses on the GPU; they will no longer be valid pointers on the host.',\n",
       "   ' However, the relative offsets of the pointers will still be valid.',\n",
       "   'Every GPU pointer'],\n",
       "  'sentences_count_spacy': 10}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1d9ad2d-e926-41b2-9d14-28303672030e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page no</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>sentences_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>311.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>311.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>135.000000</td>\n",
       "      <td>1549.691318</td>\n",
       "      <td>332.655949</td>\n",
       "      <td>12.717042</td>\n",
       "      <td>387.422830</td>\n",
       "      <td>9.691318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.922189</td>\n",
       "      <td>666.503734</td>\n",
       "      <td>135.338884</td>\n",
       "      <td>35.824532</td>\n",
       "      <td>166.625933</td>\n",
       "      <td>9.624627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.500000</td>\n",
       "      <td>1189.000000</td>\n",
       "      <td>255.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>297.250000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>135.000000</td>\n",
       "      <td>1531.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>382.750000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>212.500000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>415.500000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>494.250000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>3329.000000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>832.250000</td>\n",
       "      <td>113.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          page no  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count  311.000000       311.000000       311.000000               311.000000   \n",
       "mean   135.000000      1549.691318       332.655949                12.717042   \n",
       "std     89.922189       666.503734       135.338884                35.824532   \n",
       "min    -20.000000         0.000000         1.000000                 1.000000   \n",
       "25%     57.500000      1189.000000       255.500000                 5.000000   \n",
       "50%    135.000000      1531.000000       357.000000                 8.000000   \n",
       "75%    212.500000      1977.000000       415.500000                13.000000   \n",
       "max    290.000000      3329.000000       798.000000               499.000000   \n",
       "\n",
       "       page_token_count  sentences_count_spacy  \n",
       "count        311.000000             311.000000  \n",
       "mean         387.422830               9.691318  \n",
       "std          166.625933               9.624627  \n",
       "min            0.000000               0.000000  \n",
       "25%          297.250000               5.000000  \n",
       "50%          382.750000               8.000000  \n",
       "75%          494.250000              13.000000  \n",
       "max          832.250000             113.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0bd33-b39f-4222-82c0-de58b7067b49",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n",
    "- splitting larger texts into smaller ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1934893a-e9c8-4ab6-a2cd-f30073ce69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split size to turn group of sentences into chunks\n",
    "chunk_size = 9\n",
    "\n",
    "def split_text(input_list: list, \n",
    "              slice_size: int = chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "362f41e3-8673-4b1b-9730-227233c52dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       " [9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       " [18, 19, 20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = list(range(25))\n",
    "split_text(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68e40a6a-c6ee-47be-85be-24edba0e8ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 311/311 [00:00<00:00, 154169.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# chunking our pages\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_text(input_list=item[\"sentences\"],\n",
    "                                        slice_size = chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38aef6e8-58f7-4289-a1d5-8b90d57173e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page no': 190,\n",
       "  'page_char_count': 1611,\n",
       "  'page_word_count': 408,\n",
       "  'page_sentence_count_raw': 8,\n",
       "  'page_token_count': 402.75,\n",
       "  'text': 'streAms 190             HANDLE_ERROR( cudaMemcpy( a, dev_a,                                   size * sizeof( *a ),                                   cudaMemcpyDeviceToHost ) );     }     HANDLE_ERROR( cudaEventRecord( stop, 0 ) );     HANDLE_ERROR( cudaEventSynchronize( stop ) );     HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,                                         start, stop ) );     HANDLE_ERROR( cudaFreeHost( a ) );     HANDLE_ERROR( cudaFree( dev_a ) );     HANDLE_ERROR( cudaEventDestroy( start ) );     HANDLE_ERROR( cudaEventDestroy( stop ) );     return elapsedTime;  } As you can see, the buffer allocated by cudaHostAlloc() is used in the same  way as a buffer allocated by malloc(). The other change from using malloc() lies in the last argument, the value cudaHostAllocDefault. This last argu- ment stores a collection of flags that we can use to modify the behavior of  cudaHostAlloc() in order to allocate other varieties of pinned host memory.  In the next chapter, we’ll see how to use the other possible values of these flags,  but for now we’re content to use the default, page-locked memory so we pass  cudaHostAllocDefault in order to get the default behavior. To free a buffer  that was allocated with cudaHostAlloc(), we have to use cudaFreeHost().  That is, every malloc() needs a free(), and every cudaHostAlloc() needs  a cudaFreeHost(). The body of main() proceeds not unlike what you would expect. #include \"../common/book.h\" #define SIZE    (10*1024*1024) int main( void ) {     float           elapsedTime;     float           MB = (float)100*SIZE*sizeof(int)/1024/1024;',\n",
       "  'sentences': ['streAms 190             HANDLE_ERROR( cudaMemcpy( a, dev_a,                                   size * sizeof( *a ),                                   cudaMemcpyDeviceToHost ) );     }     HANDLE_ERROR( cudaEventRecord( stop, 0 ) );     HANDLE_ERROR( cudaEventSynchronize( stop ) );     HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,                                         start, stop ) );     HANDLE_ERROR( cudaFreeHost( a ) );     HANDLE_ERROR( cudaFree( dev_a ) );     HANDLE_ERROR( cudaEventDestroy( start ) );     HANDLE_ERROR( cudaEventDestroy( stop ) );     return elapsedTime;  } As you can see, the buffer allocated by cudaHostAlloc() is used in the same  way as a buffer allocated by malloc().',\n",
       "   'The other change from using malloc() lies in the last argument, the value cudaHostAllocDefault.',\n",
       "   'This last argu- ment stores a collection of flags that we can use to modify the behavior of  cudaHostAlloc() in order to allocate other varieties of pinned host memory.',\n",
       "   ' In the next chapter, we’ll see how to use the other possible values of these flags,  but for now we’re content to use the default, page-locked memory so we pass  cudaHostAllocDefault in order to get the default behavior.',\n",
       "   'To free a buffer  that was allocated with cudaHostAlloc(), we have to use cudaFreeHost().',\n",
       "   ' That is, every malloc() needs a free(), and every cudaHostAlloc() needs  a cudaFreeHost().',\n",
       "   'The body of main() proceeds not unlike what you would expect. #',\n",
       "   'include \"../common/book.h\" #define SIZE    (10*1024*1024) int main( void ) {     float           elapsedTime;     float           MB = (float)100*SIZE*sizeof(int)/1024/1024;'],\n",
       "  'sentences_count_spacy': 8,\n",
       "  'sentence_chunks': [['streAms 190             HANDLE_ERROR( cudaMemcpy( a, dev_a,                                   size * sizeof( *a ),                                   cudaMemcpyDeviceToHost ) );     }     HANDLE_ERROR( cudaEventRecord( stop, 0 ) );     HANDLE_ERROR( cudaEventSynchronize( stop ) );     HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,                                         start, stop ) );     HANDLE_ERROR( cudaFreeHost( a ) );     HANDLE_ERROR( cudaFree( dev_a ) );     HANDLE_ERROR( cudaEventDestroy( start ) );     HANDLE_ERROR( cudaEventDestroy( stop ) );     return elapsedTime;  } As you can see, the buffer allocated by cudaHostAlloc() is used in the same  way as a buffer allocated by malloc().',\n",
       "    'The other change from using malloc() lies in the last argument, the value cudaHostAllocDefault.',\n",
       "    'This last argu- ment stores a collection of flags that we can use to modify the behavior of  cudaHostAlloc() in order to allocate other varieties of pinned host memory.',\n",
       "    ' In the next chapter, we’ll see how to use the other possible values of these flags,  but for now we’re content to use the default, page-locked memory so we pass  cudaHostAllocDefault in order to get the default behavior.',\n",
       "    'To free a buffer  that was allocated with cudaHostAlloc(), we have to use cudaFreeHost().',\n",
       "    ' That is, every malloc() needs a free(), and every cudaHostAlloc() needs  a cudaFreeHost().',\n",
       "    'The body of main() proceeds not unlike what you would expect. #',\n",
       "    'include \"../common/book.h\" #define SIZE    (10*1024*1024) int main( void ) {     float           elapsedTime;     float           MB = (float)100*SIZE*sizeof(int)/1024/1024;']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65862421-7648-4afc-bdcc-9d3862bdaa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page no</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>sentences_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>311.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>135.000</td>\n",
       "      <td>1549.691</td>\n",
       "      <td>332.656</td>\n",
       "      <td>12.717</td>\n",
       "      <td>387.423</td>\n",
       "      <td>9.691</td>\n",
       "      <td>1.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.922</td>\n",
       "      <td>666.504</td>\n",
       "      <td>135.339</td>\n",
       "      <td>35.825</td>\n",
       "      <td>166.626</td>\n",
       "      <td>9.625</td>\n",
       "      <td>1.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-20.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.500</td>\n",
       "      <td>1189.000</td>\n",
       "      <td>255.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>297.250</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>135.000</td>\n",
       "      <td>1531.000</td>\n",
       "      <td>357.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>382.750</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>212.500</td>\n",
       "      <td>1977.000</td>\n",
       "      <td>415.500</td>\n",
       "      <td>13.000</td>\n",
       "      <td>494.250</td>\n",
       "      <td>13.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>290.000</td>\n",
       "      <td>3329.000</td>\n",
       "      <td>798.000</td>\n",
       "      <td>499.000</td>\n",
       "      <td>832.250</td>\n",
       "      <td>113.000</td>\n",
       "      <td>13.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page no  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count  311.000          311.000          311.000                  311.000   \n",
       "mean   135.000         1549.691          332.656                   12.717   \n",
       "std     89.922          666.504          135.339                   35.825   \n",
       "min    -20.000            0.000            1.000                    1.000   \n",
       "25%     57.500         1189.000          255.500                    5.000   \n",
       "50%    135.000         1531.000          357.000                    8.000   \n",
       "75%    212.500         1977.000          415.500                   13.000   \n",
       "max    290.000         3329.000          798.000                  499.000   \n",
       "\n",
       "       page_token_count  sentences_count_spacy  num_chunks  \n",
       "count           311.000                311.000     311.000  \n",
       "mean            387.423                  9.691       1.550  \n",
       "std             166.626                  9.625       1.064  \n",
       "min               0.000                  0.000       0.000  \n",
       "25%             297.250                  5.000       1.000  \n",
       "50%             382.750                  8.000       1.000  \n",
       "75%             494.250                 13.000       2.000  \n",
       "max             832.250                113.000      13.000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a72d6749-f760-4b65-a6b9-5f042d05a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 311/311 [00:00<00:00, 15065.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting chunks into separate items and joining sentences together\n",
    "import re\n",
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page no\"]\n",
    "\n",
    "        # join list of sentences into a paragraph\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" => \". A\"\n",
    "\n",
    "        \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        chunk_dict[\"chunk_character_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 \n",
    "\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab7827e0-a05c-4141-a23a-94fbe4fe14e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 261,\n",
       "  'sentence_chunk': '261 able aardvark avocado aardvark avocado Figure A.3 Resolving the conflict when adding the word aardvark Armed with some background on the notions of a hash function and collision reso- lution, we’re ready to take a look at implementing our own hash table. A CPU HASH TABLE A.2.2 As described in the previous section, our hash table will consist of essentially two parts: a hash function and a data structure of buckets. Our buckets will be imple- mented exactly as before: We will allocate an array of length N, and each entry in the array holds a list of key/value pairs. Before concerning ourselves with a hash function, we will take a look at the data structures involved: #include \"../common/book.h\" struct Entry {   unsigned int  key;   void*      value;   Entry      *next; }; struct Table {   size_t count;   Entry  **entries;   Entry  *pool;   Entry  *firstFree; };',\n",
       "  'chunk_character_count': 876,\n",
       "  'chunk_word_count': 171,\n",
       "  'chunk_token_count': 219.0}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e575a854-1d72-4b62-a516-d6299cb27b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_character_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>482.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>482.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>122.460581</td>\n",
       "      <td>963.553942</td>\n",
       "      <td>178.643154</td>\n",
       "      <td>240.888485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>93.459807</td>\n",
       "      <td>576.915757</td>\n",
       "      <td>103.398394</td>\n",
       "      <td>144.228939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.250000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>111.250000</td>\n",
       "      <td>144.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>1055.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>263.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>204.750000</td>\n",
       "      <td>1309.500000</td>\n",
       "      <td>247.750000</td>\n",
       "      <td>327.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>3265.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>816.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_character_count  chunk_word_count  chunk_token_count\n",
       "count   482.000000             482.000000        482.000000         482.000000\n",
       "mean    122.460581             963.553942        178.643154         240.888485\n",
       "std      93.459807             576.915757        103.398394         144.228939\n",
       "min     -19.000000               3.000000          1.000000           0.750000\n",
       "25%      39.250000             578.000000        111.250000         144.500000\n",
       "50%     118.000000            1055.000000        187.000000         263.750000\n",
       "75%     204.750000            1309.500000        247.750000         327.375000\n",
       "max     290.000000            3265.000000        509.000000         816.250000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "024ecd00-3933-4b37-99ce-7c45980060ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count:8.5 | Text: This page intentionally left blank\n",
      "Chunk token count:4.25 | Text: About the Authors\n",
      "Chunk token count:8.5 | Text: This page intentionally left blank\n",
      "Chunk token count:3.75 | Text: CUDA by Example\n",
      "Chunk token count:4.5 | Text: . . . .11 Contents\n"
     ]
    }
   ],
   "source": [
    "# chunks with lower tokens than 30\n",
    "min_chunk_token = 30\n",
    "for row in df[df['chunk_token_count'] <= min_chunk_token].sample(5).iterrows():\n",
    "    print(f'Chunk token count:{row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c6963710-fc1c-47f2-b8ed-ecba8976316c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -17,\n",
       "  'sentence_chunk': 'CUDA by Example g JAson sAnders edwArd KAndrot Upper Saddle River, NJ • Boston • Indianapolis • San Francisco New York • Toronto • Montreal • London • Munich • Paris • Madrid Capetown • Sydney • Tokyo • Singapore • Mexico City',\n",
       "  'chunk_character_count': 226,\n",
       "  'chunk_word_count': 43,\n",
       "  'chunk_token_count': 56.5},\n",
       " {'page_number': -16,\n",
       "  'sentence_chunk': 'Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals. The authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein. NVIDIA makes no warranty or representation that the techniques described herein are free from any Intellectual Property claims. The reader assumes all risk of any such claims based on his or her use of these techniques. The publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special sales, which may include electronic versions and/or custom covers and content particular to your business, training goals, marketing focus, and branding interests. For more information, please contact: U. S. Corporate and Government Sales (800) 382-3419 corpsales@pearsontechgroup.com For sales outside the United States, please contact: International Sales international@pearson.com Visit us on the Web: informit.com/aw Library of Congress Cataloging-in-Publication Data Sanders, Jason.  CUDA by example : an introduction to general-purpose GPU programming / Jason Sanders, Edward Kandrot.',\n",
       "  'chunk_character_count': 1556,\n",
       "  'chunk_word_count': 231,\n",
       "  'chunk_token_count': 389.0}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_enough_tokens = df[df[\"chunk_token_count\"] > min_chunk_token].to_dict(orient=\"records\")\n",
    "pages_and_chunks_enough_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c50cdd42-9738-4370-bba1-50bbecc3ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_and_chunks_enough_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef770b-2888-40b1-b915-8a01aaec28a3",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "- chunks to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2279a5e8-459b-4794-8f99-628360bac79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\After\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path = 'all-mpnet-base-v2',\n",
    "                                     device='cpu')\n",
    "\n",
    "sentences = [\"I am a person.\",\n",
    "            \"I am from Nepal.\",\n",
    "            \"I am from Earth.\"]\n",
    "\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1793ef3-12c9-4fa6-9de2-316c66beb976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am a person.\n",
      "Embedding: (768,)\n"
     ]
    }
   ],
   "source": [
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(f'Sentence: {sentence}')\n",
    "    print(f'Embedding: {embedding.shape}')\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ba5ebeb1-577b-4daf-8622-9bd6fa12e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 426/426 [00:57<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 43s\n",
      "Wall time: 58.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to('cpu')\n",
    "\n",
    "for item in tqdm(pages_and_chunks_enough_tokens):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "52fe5d0f-a714-462f-b8c2-6246af440488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 426/426 [00:07<00:00, 53.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 4s\n",
      "Wall time: 8.25 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to('cuda')\n",
    "\n",
    "for item in tqdm(pages_and_chunks_enough_tokens):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ed76a714-c46d-4fa9-9b9a-29dec96e20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating batches\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_enough_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c7f3a487-115c-40c5-b9f5-cfbccfc12edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.3 s\n",
      "Wall time: 6.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([426, 768])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                              batch_size=32,\n",
    "                                              convert_to_tensor=True)\n",
    "\n",
    "text_chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "407b91ff-8fbb-4cb2-9263-7a33c8c2b67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.5104e-04,  8.7808e-03, -2.3617e-02, -3.9672e-03, -4.7210e-02,\n",
       "         2.3732e-02,  3.2628e-02,  1.6957e-02,  1.4056e-02,  4.8171e-03,\n",
       "         3.8954e-02, -2.2313e-03,  1.6538e-02,  1.4528e-02, -2.6746e-02,\n",
       "         3.1120e-03,  1.9372e-02,  8.3560e-03,  1.0736e-02, -5.7314e-03,\n",
       "         3.8960e-02, -2.5745e-02,  2.3988e-02,  2.2226e-02, -1.6475e-02,\n",
       "        -2.9316e-02, -2.3492e-02,  3.1946e-02,  4.3804e-02,  6.8989e-02,\n",
       "        -6.8026e-02, -5.6173e-03,  1.9286e-03,  1.8211e-02,  2.1097e-06,\n",
       "         5.0570e-02, -5.4959e-02,  2.4807e-02,  4.5095e-02, -2.7462e-02,\n",
       "         4.5699e-02, -5.6143e-02, -1.8512e-02, -3.8574e-02, -1.4848e-02,\n",
       "         2.0480e-02, -1.6504e-02,  1.6074e-02, -4.6710e-02, -6.5593e-02,\n",
       "         8.8646e-03,  8.5863e-02, -1.9725e-02,  2.8828e-02, -2.8797e-02,\n",
       "         3.2957e-02, -5.0934e-02,  1.3347e-02,  6.6375e-03, -6.0752e-02,\n",
       "        -4.6561e-02,  5.3337e-02,  8.6391e-04, -3.8480e-02,  1.6685e-02,\n",
       "         1.2802e-02, -1.1374e-02,  1.0626e-02,  6.6767e-03, -3.8926e-02,\n",
       "        -1.4552e-01, -1.8258e-02, -1.0658e-02, -2.4539e-02, -6.4496e-02,\n",
       "        -2.5499e-03,  1.5610e-02, -9.3327e-03,  4.8129e-03, -2.7778e-02,\n",
       "        -1.2719e-02, -2.7599e-02,  4.1030e-02, -5.1296e-02,  2.9407e-02,\n",
       "        -8.5471e-03,  3.5800e-02,  1.3808e-02, -1.6456e-02,  5.8222e-02,\n",
       "        -1.7805e-03, -7.7026e-02,  3.5428e-02, -6.0941e-02, -3.0678e-02,\n",
       "        -1.2265e-02, -1.9358e-02, -3.3741e-02, -1.2215e-02,  2.9087e-02,\n",
       "         5.4180e-02,  5.2664e-02,  6.0486e-02, -3.3837e-02, -3.0700e-02,\n",
       "        -6.1848e-02, -5.5861e-02, -2.6109e-02, -8.5486e-03, -3.0492e-03,\n",
       "        -1.5133e-02, -3.1866e-02,  4.4490e-02,  2.4145e-02, -2.0903e-02,\n",
       "        -3.1029e-02,  3.4255e-03,  1.0767e-02, -3.5708e-02,  1.3955e-03,\n",
       "        -2.1844e-02,  1.0259e-02,  6.0780e-02,  3.0033e-02,  4.9924e-03,\n",
       "         8.4718e-03, -5.8716e-02,  1.2064e-02,  4.7501e-03,  1.3668e-02,\n",
       "         2.6957e-02,  4.4842e-03, -1.7346e-02, -1.8190e-02,  1.5194e-02,\n",
       "        -6.3907e-02, -2.0253e-02, -8.5611e-03,  3.9657e-02,  1.3481e-02,\n",
       "         4.0834e-02, -3.2199e-02,  1.0772e-02,  3.8737e-02,  1.1537e-01,\n",
       "         1.2692e-02,  2.4797e-02,  9.0587e-02, -1.3539e-02,  1.2090e-02,\n",
       "        -3.1129e-06,  1.3148e-02, -3.8872e-02, -2.0152e-02, -5.1967e-02,\n",
       "         6.1721e-03,  5.0480e-03,  2.0687e-02,  7.9019e-03, -5.0515e-03,\n",
       "        -4.7524e-02, -4.3271e-02, -3.5035e-02, -4.1776e-02,  4.1096e-02,\n",
       "        -1.1830e-02,  1.0691e-02,  2.7001e-02,  4.6665e-02,  5.5325e-02,\n",
       "        -1.7483e-02,  7.2342e-02, -2.7956e-02, -6.0553e-03,  3.0577e-02,\n",
       "        -1.6148e-02, -7.7847e-03,  3.1006e-02, -3.9798e-02, -1.6764e-02,\n",
       "         8.5629e-04,  7.7674e-02,  4.3134e-02, -2.9979e-02, -6.6798e-03,\n",
       "        -3.2159e-03,  9.1289e-02,  1.0730e-02,  2.6992e-02,  3.3279e-02,\n",
       "         2.5615e-02, -1.8137e-02,  4.0997e-02,  1.0333e-02, -2.4436e-02,\n",
       "         1.1722e-02,  2.9868e-04, -2.0302e-02,  1.6938e-02, -4.9398e-02,\n",
       "        -5.6754e-02, -1.7567e-02, -2.4738e-02,  1.5235e-02,  4.7396e-02,\n",
       "         5.3653e-02, -1.7183e-02, -2.9411e-02, -2.5797e-02, -4.3627e-02,\n",
       "        -5.6411e-02, -8.7852e-03,  1.5819e-02, -8.9181e-03,  1.1501e-02,\n",
       "         1.6077e-02, -3.4209e-02, -3.7468e-02,  6.2900e-03, -7.5687e-03,\n",
       "        -3.8122e-02,  2.7508e-02, -5.2783e-03, -3.6926e-02,  4.7921e-02,\n",
       "         5.1927e-02, -2.7482e-02,  2.3775e-02, -1.3985e-02,  1.3620e-02,\n",
       "         1.1241e-02, -4.0863e-02,  4.7169e-02, -3.0958e-02, -6.1219e-03,\n",
       "        -4.1396e-02, -8.1306e-03,  6.7899e-02,  9.1611e-03,  3.5402e-02,\n",
       "         8.2954e-03, -3.3316e-02, -4.2491e-03, -5.3704e-02,  6.1973e-03,\n",
       "         2.4842e-02,  3.5927e-02,  3.1545e-03, -8.3015e-03, -2.6837e-02,\n",
       "         3.6600e-02, -3.5674e-03,  2.0929e-02,  8.0842e-03, -8.5587e-03,\n",
       "         4.8572e-02,  2.3320e-03,  1.8137e-02, -3.7253e-02,  3.2755e-02,\n",
       "         5.1198e-02,  3.6873e-02, -1.9218e-02,  2.2860e-02, -6.1296e-02,\n",
       "        -2.8996e-02,  2.5747e-02, -1.0327e-01, -5.1633e-04,  4.9492e-03,\n",
       "         2.8677e-02,  5.2127e-02, -7.5821e-03, -9.9455e-03, -1.9933e-02,\n",
       "         5.4680e-02,  7.5292e-02, -1.0364e-02,  2.0810e-02,  1.5052e-02,\n",
       "         4.4943e-02,  8.2747e-03,  4.8049e-02, -2.3265e-02, -1.2763e-03,\n",
       "         6.5498e-02,  1.8394e-02,  2.4278e-03,  4.4220e-02, -2.4325e-02,\n",
       "        -1.3069e-02,  1.4676e-02,  3.2973e-02,  5.6765e-02,  9.9326e-03,\n",
       "         7.7872e-03,  1.1520e-02, -2.7047e-02, -7.8197e-03, -9.2139e-03,\n",
       "         1.1538e-02, -3.9532e-02,  2.8950e-03,  7.8226e-03,  3.7078e-02,\n",
       "        -1.2099e-02,  4.7683e-02,  2.5133e-02, -4.8656e-02, -2.0114e-02,\n",
       "        -2.2470e-02, -1.3046e-02, -3.5753e-02, -1.0509e-02,  3.5333e-02,\n",
       "        -1.5692e-02, -4.3552e-02,  3.4593e-03,  6.4470e-03,  1.3538e-03,\n",
       "        -1.9005e-03, -2.2301e-02, -2.7164e-02, -4.5423e-02,  1.4754e-02,\n",
       "         5.8122e-03,  5.4744e-02, -3.0534e-02, -5.1729e-02,  3.2276e-02,\n",
       "         6.6266e-03, -9.4956e-02,  5.2490e-02,  1.0774e-02,  3.3651e-02,\n",
       "        -1.3224e-02,  1.3738e-02, -2.7087e-02,  2.1762e-02,  3.3949e-02,\n",
       "        -5.0520e-02, -1.0613e-04,  3.3282e-02,  6.3168e-02,  3.5508e-02,\n",
       "        -4.4946e-03,  2.2265e-02,  1.3043e-02,  6.6335e-02,  1.7490e-02,\n",
       "        -2.8320e-02,  1.3587e-02, -1.7112e-02, -5.3348e-02,  2.5570e-02,\n",
       "        -1.1623e-02, -4.4252e-03, -2.9657e-02, -2.3416e-02,  7.4365e-02,\n",
       "        -1.1718e-02,  7.3179e-02,  1.2191e-01, -3.5206e-02, -7.8202e-02,\n",
       "         1.3587e-02,  2.4500e-02,  2.4813e-02, -3.3040e-02,  6.8186e-03,\n",
       "        -3.8156e-02, -4.7876e-02,  6.2078e-03, -2.6533e-02, -1.8728e-02,\n",
       "        -1.1102e-02, -1.3632e-02,  1.0587e-02,  4.7315e-02, -1.5431e-02,\n",
       "         5.6874e-02, -8.9803e-02, -8.0416e-03,  1.0048e-02,  4.3170e-03,\n",
       "        -4.3553e-02, -1.7562e-02,  5.2384e-02,  3.9599e-02,  3.3701e-02,\n",
       "        -9.5558e-03, -4.8538e-02,  2.1758e-02,  1.7333e-02,  1.5350e-02,\n",
       "         9.7730e-03,  1.3191e-02, -2.4978e-02, -6.9353e-03,  3.1630e-02,\n",
       "        -2.2017e-02,  1.3858e-02,  5.5280e-02, -6.8213e-03,  9.2317e-03,\n",
       "        -8.4314e-02,  5.5227e-02,  1.7715e-02,  3.7569e-02,  3.2115e-03,\n",
       "         4.2975e-02,  7.9874e-02,  2.0545e-02, -5.0443e-02, -4.3427e-02,\n",
       "        -3.8167e-02,  6.3870e-03, -1.0572e-02, -9.2191e-02,  4.4580e-02,\n",
       "        -4.5475e-03, -2.9201e-02,  6.6128e-02, -1.4949e-02, -4.6251e-02,\n",
       "         1.9134e-02, -4.7056e-02,  1.9117e-02, -1.7981e-02, -1.7506e-02,\n",
       "         1.2414e-02, -7.5049e-02, -1.1544e-02, -1.3785e-02,  2.5349e-02,\n",
       "         8.4302e-03, -3.5222e-02,  1.0831e-01, -8.9711e-03,  6.1954e-02,\n",
       "        -3.6274e-02, -2.2218e-02,  6.1098e-02,  7.9358e-03,  6.5209e-02,\n",
       "        -2.2464e-02,  8.3378e-03, -4.8200e-02,  2.4586e-02, -2.3738e-02,\n",
       "         2.8088e-03,  2.7942e-02, -6.3899e-03, -5.2644e-02, -2.5957e-02,\n",
       "        -7.3536e-03, -2.3754e-02, -1.9534e-03,  2.0722e-02,  3.6617e-02,\n",
       "         6.9712e-03, -6.3457e-04,  1.9630e-02,  2.7354e-02, -3.3587e-02,\n",
       "        -1.2802e-02,  1.1657e-02,  1.0772e-02,  1.2524e-02, -5.9707e-02,\n",
       "        -8.9915e-02, -7.3320e-03, -4.0237e-02,  4.4578e-02, -5.1829e-02,\n",
       "         8.2116e-02,  6.3128e-04,  6.8624e-02,  6.2719e-02, -3.4550e-02,\n",
       "         3.0955e-02,  5.0729e-02, -7.3371e-03,  1.2517e-02, -6.8712e-03,\n",
       "         2.3173e-02, -1.8142e-03,  2.7937e-02, -1.6687e-02, -8.6197e-02,\n",
       "        -1.5556e-02,  4.7100e-02, -2.7363e-02,  3.2648e-02,  1.3659e-02,\n",
       "        -1.3448e-02, -3.6560e-02,  1.8151e-02, -2.0762e-02,  2.1111e-02,\n",
       "         8.7151e-03, -2.2909e-02,  4.4268e-03, -2.2233e-03,  2.1923e-02,\n",
       "         6.1516e-02, -3.6104e-02,  3.2037e-02, -1.4498e-02, -2.5591e-02,\n",
       "         1.0380e-02,  5.5279e-02, -1.2705e-02, -6.4147e-02, -2.4983e-02,\n",
       "        -5.2035e-02,  3.7960e-03, -8.4993e-03, -3.9143e-02, -7.4684e-03,\n",
       "         1.7310e-03, -1.8571e-02, -1.4690e-03,  9.1451e-03, -1.9165e-02,\n",
       "         5.2066e-03, -3.1597e-02, -3.7634e-02,  1.0835e-02, -3.6745e-02,\n",
       "        -3.7292e-02, -3.8253e-02,  2.3529e-03,  7.9028e-04, -6.2633e-03,\n",
       "        -4.9554e-02, -1.4800e-02,  4.1569e-02, -1.2724e-02, -3.6929e-02,\n",
       "         1.4524e-02, -2.5414e-02, -2.1263e-02, -5.6998e-03, -2.8773e-02,\n",
       "        -1.3357e-02,  2.3352e-02,  1.6489e-02,  8.2322e-04, -5.0142e-02,\n",
       "         6.5898e-03,  7.3499e-02,  6.8124e-02,  5.4031e-02,  2.7756e-02,\n",
       "        -5.9723e-33, -1.2676e-02, -2.7906e-02,  1.5052e-02, -1.0087e-01,\n",
       "        -1.1167e-02, -2.0902e-02, -1.7808e-02, -6.3552e-03,  1.9928e-02,\n",
       "        -1.3499e-02, -4.6897e-02,  6.7278e-03,  7.7801e-03,  1.3431e-02,\n",
       "         2.5353e-02, -5.6207e-02,  1.3696e-02,  1.1206e-02,  8.7639e-03,\n",
       "         3.8002e-03,  3.3550e-03,  2.4275e-02, -4.5747e-02, -1.9092e-02,\n",
       "        -4.7529e-02, -1.6050e-02,  3.5961e-02, -5.8482e-02, -4.1302e-02,\n",
       "        -2.4508e-02, -2.5102e-02, -1.8687e-02, -2.1461e-02, -5.8602e-02,\n",
       "         6.2799e-04, -1.7490e-03, -3.7193e-02,  4.0850e-02,  4.8447e-02,\n",
       "         2.7708e-02, -4.7704e-02, -1.7495e-02, -1.7070e-02,  6.6906e-02,\n",
       "        -2.8041e-02, -2.4035e-02, -9.5239e-02, -1.3788e-02, -7.9488e-02,\n",
       "        -3.8890e-02, -3.9421e-02,  8.8009e-03,  2.5870e-02,  1.2139e-01,\n",
       "         9.1532e-02, -5.2859e-02, -2.8970e-02,  2.2460e-02, -5.1371e-02,\n",
       "        -1.6798e-02, -2.1678e-02, -3.8059e-02,  5.8783e-02, -3.8309e-02,\n",
       "         3.9578e-03,  3.7369e-03,  8.1762e-02, -1.7332e-03, -2.0570e-02,\n",
       "         6.7573e-02, -3.1860e-02, -8.0416e-02, -3.0436e-03, -3.4285e-02,\n",
       "         4.8439e-02,  1.3907e-02, -4.0523e-02, -1.8421e-02,  4.2498e-03,\n",
       "        -2.2886e-03,  2.5321e-02,  6.0950e-02, -3.6894e-02, -2.5505e-02,\n",
       "         5.3030e-02,  5.0398e-03, -2.6851e-03, -2.2744e-02,  1.0113e-02,\n",
       "         2.6938e-02,  6.5744e-03,  6.2712e-02, -9.5900e-03,  2.4671e-02,\n",
       "         6.7437e-02, -1.1391e-02,  2.3441e-02, -4.0075e-02,  4.2829e-02,\n",
       "         2.3686e-03,  6.6495e-03, -1.7239e-02, -1.0883e-02, -5.6626e-02,\n",
       "         3.9620e-02, -4.5048e-02, -3.2769e-03,  1.4897e-02, -3.5770e-02,\n",
       "         2.9454e-03, -1.9558e-02,  5.3726e-02, -2.9874e-03, -1.0909e-02,\n",
       "         1.2745e-02,  6.8096e-03,  2.1475e-02,  2.0727e-02,  2.7151e-03,\n",
       "         1.5798e-02, -1.3373e-02,  4.7684e-02, -3.0921e-02, -3.8432e-02,\n",
       "         6.0025e-02,  4.2692e-02,  1.9426e-02, -4.1773e-02, -1.5938e-02,\n",
       "        -5.4232e-02, -1.0905e-02,  2.0462e-02,  2.9035e-07,  1.9656e-02,\n",
       "         4.5344e-02, -1.7645e-02, -3.3982e-02, -8.0113e-03,  2.6992e-02,\n",
       "         8.1750e-03, -8.7198e-03,  2.4778e-02,  5.9131e-02,  1.5769e-02,\n",
       "        -9.8920e-03,  2.7533e-02, -6.4975e-03,  1.3179e-02, -3.2668e-02,\n",
       "        -7.5601e-03, -1.5359e-02,  4.3867e-03,  4.5530e-03, -8.1846e-02,\n",
       "         1.0772e-02, -4.6638e-02,  9.7010e-03,  1.0129e-01,  5.6571e-02,\n",
       "        -1.1171e-03, -1.2788e-03,  1.3940e-02, -2.4962e-02,  1.4604e-02,\n",
       "        -1.6401e-03, -6.7767e-02,  5.2040e-02,  1.9149e-02,  2.1144e-03,\n",
       "        -8.5025e-03,  2.2987e-03, -3.2746e-02,  2.4899e-02,  1.5826e-02,\n",
       "        -7.3814e-02,  2.2242e-02, -2.9297e-02,  5.3843e-02,  8.4946e-02,\n",
       "         8.5306e-03, -5.0383e-02,  6.4305e-02,  1.4152e-02,  4.0659e-02,\n",
       "         1.4538e-02,  4.8365e-02,  2.0738e-02,  1.6166e-02,  4.4487e-03,\n",
       "        -2.8230e-02, -6.2400e-03, -1.9200e-02,  4.2645e-02, -5.9015e-02,\n",
       "        -1.1405e-02, -2.7826e-02,  9.8790e-02,  3.7568e-02, -6.0011e-02,\n",
       "        -4.5955e-02,  3.0106e-34, -1.3515e-02,  2.7999e-02,  4.8108e-02,\n",
       "        -1.6663e-02, -1.8132e-03, -3.4662e-02, -5.5639e-02, -5.3846e-02,\n",
       "        -4.9820e-02,  1.2010e-03, -1.5315e-03], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk_embeddings[412]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d88cda-a934-47e9-a590-1b03f03b951b",
   "metadata": {},
   "source": [
    "#### Saving the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cd638dae-a28a-4be2-b4c3-896162460506",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embeddings_df = pd.DataFrame(pages_and_chunks_enough_tokens)\n",
    "df_path = \"text_chunks_embeddings_df.csv\"\n",
    "\n",
    "chunks_embeddings_df.to_csv(df_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
